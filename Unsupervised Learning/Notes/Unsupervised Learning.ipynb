{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b763f6",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab547668",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Types of Unsupervised Learning\n",
    "There are two popular methods for unsupervised machine learning.\n",
    "\n",
    "1. Clustering - which groups data together based on similarities\n",
    "\n",
    "2. Dimensionality Reduction - which condenses a large number of features into a (usually much) smaller set of features.\n",
    "\n",
    "## K-Means\n",
    "\n",
    "The K-Means algorithm is used to cluster all sorts of data.\n",
    "\n",
    "It can group together\n",
    "\n",
    "1. Books of similar genres or written by the same authors.\n",
    "2. Similar movies.\n",
    "3. Similar music.\n",
    "4. Similar groups of customers.\n",
    "\n",
    "This clustering can lead to product, movie, music and other types of recommendations.\n",
    "\n",
    "In the K-means algorithm __'k' represents the number of clusters you have in your dataset__.\n",
    "\n",
    "### Elbow Method\n",
    "When you have no idea how many clusters exist in your dataset, a common strategy for determining __k__ is the __elbow method__. In the elbow method, you create a plot of the number of clusters (on the x-axis) vs. the average distance of the center of the cluster to each point (on the y-axis). This plot is called a __scree plot__\n",
    "\n",
    "The average distance will always decrease with each additional cluster center. However, with fewer clusters, those decreases will be more substantial. At some point, adding new clusters will no longer create a substantial decrease in the average distance. This point is known as the __elbow__.\n",
    "\n",
    "![elbowMethod](./img/elbowMethod.png)\n",
    "\n",
    "### How Does K-Means Work?\n",
    "\n",
    "Here is one method for computing k-means:\n",
    "\n",
    "1. Randomly place k centroids amongst your data.\n",
    "\n",
    "Then within a loop until convergence perform the following two steps:\n",
    "\n",
    "2. Assign each point to the closest centroid.\n",
    "\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "\n",
    "At the end of this process, you should have k-clusters of points.\n",
    "\n",
    "The [blog by Naftali Harris](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) is spectacular at showing you how k-means works for a number of situations.\n",
    "\n",
    "The starting points of the centroids can actually make a difference as to the final results you obtain from the k-means algorithm.\n",
    "\n",
    "In order to assure you have the \"best\" set of clusters, the algorithm you saw earlier will be performed a few times with different starting points. The best set of clusters is then the clustering that creates the smallest average distance from each point to its corresponding centroid.\n",
    "\n",
    "## Feature Scaling\n",
    "For any machine learning algorithm that uses distances as a part of its optimization, it is important to scale your features.\n",
    "\n",
    "You saw this earlier in regularized forms of regression like Ridge and Lasso, but it is also true for k-means. In future sections on PCA and ICA, feature scaling will again be important for the successful optimization of your machine learning algorithms.\n",
    "\n",
    "Though there are a large number of ways that you can go about scaling your features, there are two ways that are most common:\n",
    "\n",
    "1. __Normalizing__ or __Max-Min Scaling__ - this type of scaling transforms variable values to between 0 and 1.\n",
    "2. __Standardizing__ or __Z-Score Scaling__ - this type of scaling transforms variable values so they have a mean of 0 and standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690bb3b",
   "metadata": {},
   "source": [
    "## Hierarchical and Density Based Clustering\n",
    "\n",
    "### Single Link Clustering\n",
    "Measure: the cloesest points between two clusterings\n",
    "\n",
    "Single linkage looks at the closest point to the cluster, that can result in clusters of various shapes.\n",
    "\n",
    "![singleLink](./img/singleLink.png)\n",
    "\n",
    "![comparison](./img/comparison.png)\n",
    "\n",
    "![dendrograms](./img/dendrograms.png)\n",
    "\n",
    "### Complete Link Clustering\n",
    "Measure: the farest points between two clusterings\n",
    "\n",
    "![completeLink](./img/completeLink.png)\n",
    "\n",
    "### Average Link Clustering\n",
    "Measure: the average distance between every point and every other point in the other cluster\n",
    "\n",
    "![averageLink](./img/averageLink.png)\n",
    "\n",
    "### Ward CLustering\n",
    "Ward's method does indeed try to minimize the variance resulting in each merging step.\n",
    "\n",
    "![ward](./img/ward.png)\n",
    "\n",
    "- Single and complete linkage follow merging heuristics that involve mainly one point. They do not pay much attention to in-cluster variance.\n",
    "\n",
    "- Ward and average linkage generally tend to result in compact clusters. One of the remaining two options doesn't.\n",
    "\n",
    "#### HC Examples and Applications\n",
    "\n",
    "![advantages](./img/advantages.png)\n",
    "\n",
    "- [Using Hierarchical Clustering of Secreted Protein Families to Classify and Rank Candidate Effectors of Rust Fungi](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029847)\n",
    "\n",
    "- [Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency](https://pubmed.ncbi.nlm.nih.gov/21129376/)\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Hierarchical Clustering\n",
    "```python\n",
    "from sklearn import datasets, cluster\n",
    "\n",
    "# Load dataset\n",
    "X = datasets.load_iris().data[:10]\n",
    "\n",
    "# Specify the parameters for the clustering. 'ward' linkage is default.\n",
    "# Can also use 'complete' or 'average'.\n",
    "clust = cluster.AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "\n",
    "labels = clust.fit_predict(X)\n",
    "\n",
    "# 'labels' now contains an array representing which cluster each point belongs to:\n",
    "# [1 0 0 0 1 2 0 1 0 0]\n",
    "```\n",
    "\n",
    "#### Dendrograms\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, single\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "X = datasets.load_iris().data[:10]\n",
    "\n",
    "# Perform clustering\n",
    "linkage_matrix = ward(X)\n",
    "\n",
    "# Plot dendogram\n",
    "dendogram(linkage_matrix)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### DBSCAN\n",
    "Density Based Spatial Clustering Applications with Noise\n",
    "\n",
    "![dbscan](./img/dbscan.png)\n",
    "\n",
    "![dbscanComparison](./img/dbscanComparison.png)\n",
    "\n",
    "Incredible [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/). Allows you to change its parameters and see how it works on various datasets. Highly recomended!\n",
    "\n",
    "#### DBSCAN Examples and Applications\n",
    "\n",
    "![dbscanAdvantages](./img/dbscanAdvantages.png)\n",
    "\n",
    "[Traffic Classification Using Clustering Algorithms](https://pages.cpsc.ucalgary.ca/~carey/papers/2007/jeff-perf2007.pdf)\n",
    "\n",
    "[Anomaly detection in temperature data using dbscan algorithm](https://ieeexplore.ieee.org/abstract/document/5946052)\n",
    "\n",
    "[Hierarchical density based clustering](https://www.researchgate.net/publication/315508524_hdbscan_Hierarchical_density_based_clustering)\n",
    "\n",
    "#### Implementation\n",
    "```python\n",
    "from sklearn import datasets, cluster\n",
    "\n",
    "# Load dataset\n",
    "X = datasets.load_iris().data\n",
    "\n",
    "# Specify the parameters for the clustering. These are the defaults.\n",
    "db = cluster.DBSCAN(eps=0.5, min_samples=5)\n",
    "db.fit(X)\n",
    "\n",
    "# 'db.labels_' now contains an array representing which cluster each point belongs to.\n",
    "# Samples labeled '-1' are noise.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058bee02",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models and Cluster Validation\n",
    "\n",
    "### Expectation Maximization (EM) Algorithm\n",
    "\n",
    "Expectation Maximization for Gaussian Mixtures:\n",
    "\n",
    "Steps:\n",
    "1. Initialize K gaussian distributions\n",
    "2. Soft-cluster data - __Expectation__\n",
    "3. Re-estimate the gaussians - __Maximization__\n",
    "4. Evaluate log-likelihood to check for convergence\n",
    "Repeat from step 2 until converged.\n",
    "\n",
    "### Example\n",
    "\n",
    "![gmmStep1](./img/gmmStep1.png)\n",
    "\n",
    "![gmmStep2](./img/gmmStep2.png)\n",
    "\n",
    "![gmmStep3](./img/gmmStep3.png)\n",
    "\n",
    "![gmmStep3Var](./img/gmmStep3Var.png)\n",
    "\n",
    "![gmmStep4](./img/gmmStep4.png)\n",
    "\n",
    "It is indeed important that we are careful in choosing the parameters of the initial Gaussians. That has a significant effect on the quality of EM's result.\n",
    "\n",
    "### Implementation\n",
    "```python\n",
    "from sklearn import datasets, mixture\n",
    "\n",
    "# Load dataset\n",
    "X = datasets.load_iris().data[:10]\n",
    "\n",
    "# Specify the parameters for the clustering\n",
    "gmm = mixture.GaussianMixture(n_components=3)\n",
    "gmm.fit(X)\n",
    "clustering = gmm.predict(X)\n",
    "\n",
    "# 'clustering' now contains an array representing which each point belongs to:\n",
    "# [1 0 0 0 1 2 0 1 0 0]\n",
    "```\n",
    "\n",
    "### Examples and Applications\n",
    "\n",
    "![gmmAdvantages](./img/gmmAdvantages.png)\n",
    "\n",
    "[Nonparametric discovery of human routines from sensor data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&rep=rep1&type=pdf)\n",
    "\n",
    "[Application of the Gaussian mixture model in pulsar astronomy](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.338&rep=rep1&type=pdf)\n",
    "\n",
    "[Speaker Verification Using Adapted Gaussian Mixture Models](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.338&rep=rep1&type=pdf)\n",
    "\n",
    "[Adaptive background mixture models for real-time tracking](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)\n",
    "\n",
    "[https://www.youtube.com/watch?v=lLt9H6RFO6A](https://www.youtube.com/watch?v=lLt9H6RFO6A)\n",
    "\n",
    "### Cluster Validation\n",
    "\n",
    "Categories:\n",
    "- External indices: data is originally labeled.\n",
    "- Internal indices\n",
    "- Relative indices\n",
    "\n",
    "Validation:\n",
    "- Compactness: measure how close the elements of a cluster are to each other\n",
    "- Separability: how far or distinct clusters are from each other\n",
    "\n",
    "#### External Validation Indices\n",
    "\n",
    "![externalIndices](./img/externalIndices.png)\n",
    "\n",
    "##### Adjusted Rand Index\n",
    "\n",
    "![ari](./img/ari.png)\n",
    "\n",
    "[Details of the Adjusted Rand index](http://faculty.washington.edu/kayee/pca/supp.pdf)\n",
    "\n",
    "#### Internal Validation Indices\n",
    "\n",
    "#####  Silhouette coefficient\n",
    "\n",
    "We can calculate the silhouette coefficient for each point, we can just average them across a cluster or an entire dataset.\n",
    "\n",
    "![silhouette](./img/silhouette.png)\n",
    "\n",
    "[Density-Based Clustering Validation](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=83C3BD5E078B1444CB26E243975507E1?doi=10.1.1.707.9034&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920555b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
