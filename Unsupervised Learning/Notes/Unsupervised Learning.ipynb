{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b763f6",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab547668",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Types of Unsupervised Learning\n",
    "There are two popular methods for unsupervised machine learning.\n",
    "\n",
    "1. Clustering - which groups data together based on similarities\n",
    "\n",
    "2. Dimensionality Reduction - which condenses a large number of features into a (usually much) smaller set of features.\n",
    "\n",
    "## K-Means\n",
    "\n",
    "The K-Means algorithm is used to cluster all sorts of data.\n",
    "\n",
    "It can group together\n",
    "\n",
    "1. Books of similar genres or written by the same authors.\n",
    "2. Similar movies.\n",
    "3. Similar music.\n",
    "4. Similar groups of customers.\n",
    "\n",
    "This clustering can lead to product, movie, music and other types of recommendations.\n",
    "\n",
    "In the K-means algorithm __'k' represents the number of clusters you have in your dataset__.\n",
    "\n",
    "### Elbow Method\n",
    "When you have no idea how many clusters exist in your dataset, a common strategy for determining __k__ is the __elbow method__. In the elbow method, you create a plot of the number of clusters (on the x-axis) vs. the average distance of the center of the cluster to each point (on the y-axis). This plot is called a __scree plot__\n",
    "\n",
    "The average distance will always decrease with each additional cluster center. However, with fewer clusters, those decreases will be more substantial. At some point, adding new clusters will no longer create a substantial decrease in the average distance. This point is known as the __elbow__.\n",
    "\n",
    "![elbowMethod](./img/elbowMethod.png)\n",
    "\n",
    "### How Does K-Means Work?\n",
    "\n",
    "Here is one method for computing k-means:\n",
    "\n",
    "1. Randomly place k centroids amongst your data.\n",
    "\n",
    "Then within a loop until convergence perform the following two steps:\n",
    "\n",
    "2. Assign each point to the closest centroid.\n",
    "\n",
    "3. Move the centroid to the center of the points assigned to it.\n",
    "\n",
    "At the end of this process, you should have k-clusters of points.\n",
    "\n",
    "The [blog by Naftali Harris](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) is spectacular at showing you how k-means works for a number of situations.\n",
    "\n",
    "The starting points of the centroids can actually make a difference as to the final results you obtain from the k-means algorithm.\n",
    "\n",
    "In order to assure you have the \"best\" set of clusters, the algorithm you saw earlier will be performed a few times with different starting points. The best set of clusters is then the clustering that creates the smallest average distance from each point to its corresponding centroid.\n",
    "\n",
    "## Feature Scaling\n",
    "For any machine learning algorithm that uses distances as a part of its optimization, it is important to scale your features.\n",
    "\n",
    "You saw this earlier in regularized forms of regression like Ridge and Lasso, but it is also true for k-means. In future sections on PCA and ICA, feature scaling will again be important for the successful optimization of your machine learning algorithms.\n",
    "\n",
    "Though there are a large number of ways that you can go about scaling your features, there are two ways that are most common:\n",
    "\n",
    "1. __Normalizing__ or __Max-Min Scaling__ - this type of scaling transforms variable values to between 0 and 1.\n",
    "2. __Standardizing__ or __Z-Score Scaling__ - this type of scaling transforms variable values so they have a mean of 0 and standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf413c",
   "metadata": {},
   "source": [
    "## Hierarchical and Density Based Clustering\n",
    "\n",
    "### Single Link Clustering\n",
    "Measure: the cloesest points between two clusterings\n",
    "\n",
    "Single linkage looks at the closest point to the cluster, that can result in clusters of various shapes.\n",
    "\n",
    "![singleLink](./img/singleLink.png)\n",
    "\n",
    "![comparison](./img/comparison.png)\n",
    "\n",
    "![dendrograms](./img/dendrograms.png)\n",
    "\n",
    "### Complete Link Clustering\n",
    "Measure: the farest points between two clusterings\n",
    "\n",
    "![completeLink](./img/completeLink.png)\n",
    "\n",
    "### Average Link Clustering\n",
    "Measure: the average distance between every point and every other point in the other cluster\n",
    "\n",
    "![averageLink](./img/averageLink.png)\n",
    "\n",
    "### Ward CLustering\n",
    "Ward's method does indeed try to minimize the variance resulting in each merging step.\n",
    "\n",
    "![ward](./img/ward.png)\n",
    "\n",
    "- Single and complete linkage follow merging heuristics that involve mainly one point. They do not pay much attention to in-cluster variance.\n",
    "\n",
    "- Ward and average linkage generally tend to result in compact clusters. One of the remaining two options doesn't.\n",
    "\n",
    "#### HC Examples and Applications\n",
    "\n",
    "![advantages](./img/advantages.png)\n",
    "\n",
    "- [Using Hierarchical Clustering of Secreted Protein Families to Classify and Rank Candidate Effectors of Rust Fungi](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029847)\n",
    "\n",
    "- [Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency](https://pubmed.ncbi.nlm.nih.gov/21129376/)\n",
    "\n",
    "### DBSCAN\n",
    "Density Based Spatial Clustering Applications with Noise\n",
    "\n",
    "![dbscan](./img/dbscan.png)\n",
    "\n",
    "![dbscanComparison](./img/dbscanComparison.png)\n",
    "\n",
    "Incredible [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/). Allows you to change its parameters and see how it works on various datasets. Highly recomended!\n",
    "\n",
    "#### DBSCAN Examples and Applications\n",
    "\n",
    "![dbscanAdvantages](./img/dbscanAdvantages.png)\n",
    "\n",
    "[Traffic Classification Using Clustering Algorithms](https://pages.cpsc.ucalgary.ca/~carey/papers/2007/jeff-perf2007.pdf)\n",
    "\n",
    "[Anomaly detection in temperature data using dbscan algorithm](https://ieeexplore.ieee.org/abstract/document/5946052)\n",
    "\n",
    "[Hierarchical density based clustering](https://www.researchgate.net/publication/315508524_hdbscan_Hierarchical_density_based_clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01347c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
