{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8045cbd",
   "metadata": {},
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcb52f",
   "metadata": {},
   "source": [
    "## Image Representation\n",
    "\n",
    "__Gradient descent__ is a a mathematical way to minimize error in a neural network. More information on this minimization method can be found [here](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "__Convolutional neural networks__ are a specific type of neural network that are commonly used in computer vision applications. They learn to recognize patterns among a given set of images. If you want to learn more, refer to [this resource](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/), and we'll be learning more about these types of networks, and how they work step-by-step, at a different point in this course!\n",
    "\n",
    "### Images as Numerical Data\n",
    "Every pixel in an image is just a numerical value and, we can also change these pixel values. We can multiply every single one by a scalar to change how bright the image is, we can shift each pixel value to the right, and many more operations!\n",
    "\n",
    "__Treating images as grids of numbers is the basis for many image processing techniques.__\n",
    "\n",
    "Most color and shape transformations are done just by mathematically operating on an image and changing it pixel-by-pixel.\n",
    "\n",
    "### Color Images\n",
    "Color images are interpreted as 3D cubes of values with width, height, and depth!\n",
    "\n",
    "The depth is the number of colors. Most color images can be represented by combinations of only 3 colors: red, green, and blue values; these are known as RGB images. And for RGB images, the depth is 3!\n",
    "\n",
    "It’s helpful to think of the depth as three stacked, 2D color layers. One layer is Red, one Green, and one Blue. Together they create a complete color image.\n",
    "\n",
    "### Importance of Color\n",
    "In general, when you think of a classification challenge, like identifying lane lines or cars or people, you can decide whether color information and color images are useful by thinking about your own vision.\n",
    "\n",
    "If the identification problem is easier in color for us humans, it’s likely easier for an algorithm to see color images too!\n",
    "\n",
    "### OpenCV\n",
    "[OpenCV](https://opencv.org/) is a popular computer vision library that has meany built in tools for image analysis and understanding!\n",
    "\n",
    "Note: In the example above and in later examples, I'm using my own Jupyter notebook and sets of images stored on my personal computer. You're encouraged to set up a similar environment and use images of your own to practice! You'll also be given some code quizzes (coming up next), with images provided, to practice these techniques.\n",
    "\n",
    "### Why BGR instead of RGB?\n",
    "OpenCV reads in images in BGR format (instead of RGB) because when OpenCV was first being developed, BGR color format was popular among camera manufacturers and image software providers. The red channel was considered one of the least important color channels, so was listed last, and many bitmaps use BGR format for image storage. However, now the standard has changed and most image software and cameras use RGB format, which is why, in these examples, it's good practice to initially convert BGR images to RGB before analyzing or manipulating them.\n",
    "\n",
    "### Changing Color Spaces\n",
    "To change color spaces, we used OpenCV's `cvtColor` function, whose documentation is [here](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html).\n",
    "\n",
    "### Color Selection\n",
    "To select the most accurate color boundaries, it's often useful to use a [color picker](https://www.w3schools.com/colors/colors_picker.asp) and choose the color boundaries that define the region you want to select!\n",
    "\n",
    "### Distinguishing and Measurable Traits\n",
    "When you approach a classification challenge, you may ask yourself: how can I tell these images apart? What traits do these images have that differentiate them, and how can I write code to represent their differences? Adding on to that, how can I ignore irrelevant or overly similar parts of these images?\n",
    "\n",
    "You may have thought about a number of distinguishing features: day images are much brighter, generally, than night images. Night images also have these really bright small spots, so the brightness over the whole image varies a lot more than the day images. There is a lot more of a gray/blue color palette in the day images.\n",
    "\n",
    "There are lots of measurable traits that distinguish these images, and these measurable traits are referred to as __features__.\n",
    "\n",
    "A feature a measurable component of an image or object that is, ideally, unique and recognizable under varying conditions - like under varying light or camera angle. And we’ll learn more about features soon.\n",
    "\n",
    "### Standardizing and Pre-processing\n",
    "But we’re getting ahead of ourselves! To extract features from any image, we have to pre-process and standardize them!\n",
    "\n",
    "Next we’ll take a look at the standardization steps we should take before we can consistently extract features\n",
    "\n",
    "### Numerical vs. Categorical\n",
    "Let's learn a little more about labels. After visualizing the image data, you'll have seen that each image has an attached label: \"day\" or \"night,\" and these are known as __categorical values__.\n",
    "\n",
    "Categorical values are typically text values that represent various traits about an image. A couple examples are:\n",
    "\n",
    "- An \"animal\" variable with the values: \"cat,\" \"tiger,\" \"hippopotamus,\" and \"dog.\"\n",
    "- A \"color\" variable with the values: \"red,\" \"green,\" and \"blue.\"\n",
    "\n",
    "Each value represents a different category, and most collected data is labeled in this way!\n",
    "\n",
    "These labels are descriptive for us, but may be inefficient for a classification task. Many machine learning algorithms do not use categorical data; they require that all output be numerical. Numbers are easily compared and stored in memory, and for this reason, we often have to convert categorical values into __numerical labels__. There are two main approaches that you'll come across:\n",
    "\n",
    "1. Integer encoding\n",
    "2. One hot-encoding\n",
    "\n",
    "### Integer Encoding\n",
    "Integer encoding means to assign each category value an integer value. So, day = 1 and night = 0. This is a nice way to separate binary data, and it's what we'll do for our day and night images.\n",
    "\n",
    "### One-hot Encoding\n",
    "One-hot encoding is often used when there are more than 2 values to separate. A one-hot label is a 1D list that's the length of the number of classes. Say we are looking at the animal variable with the values: \"cat,\" \"tiger,\" \"hippopotamus,\" and \"dog.\" There are 4 classes in this category and so our one-hot labels will be a list of length four. The list will be all 0's and one 1; the 1 indicates which class a certain image is.\n",
    "\n",
    "For example, since we have four classes (cat, tiger, hippopotamus, and dog), we can make a list in that order: [cat value, tiger value, hippopotamus value, dog value]. In general, order does not matter.\n",
    "\n",
    "If we have an image and it's one-hot label is `[0, 1, 0, 0]`, what does that indicate?\n",
    "\n",
    "In order of [cat value, tiger value, hippopotamus value, dog value], that label indicates that it's an image of a tiger! Let's do one more example, what about the label `[0, 0, 0, 1]`?\n",
    "\n",
    "### Average Brightness\n",
    "Here were the steps we took to extract the average brightness of an image.\n",
    "\n",
    "1. Convert the image to HSV color space (the Value channel is an approximation for brightness)\n",
    "2. Sum up all the values of the pixels in the Value channel\n",
    "3. Divide that brightness sum by the area of the image, which is just the width times the height.\n",
    "\n",
    "This gave us one value: the average brightness or the average Value of that image.\n",
    "\n",
    "In the next notebook, make sure to look at a variety of day and night images and see if you can think of an average brightness value that will separate the images into their respective classes!\n",
    "\n",
    "The next step will be to feed this data into a classifier. A classifier might be as simple as a conditional statement that checks if the average brightness is above some threshold, then this image is labeled as 1 (day) and if not, it’s labeled as 0 (night).\n",
    "\n",
    "On your own, you can choose to create more features that help distinguish these images from one another, and we’ll soon learn about testing the accuracy of a model like this.\n",
    "\n",
    "### Classification Task\n",
    "Let’s now complete our day and night classifier. After we extracted the average brightness value, we want to turn this feature into a `predicted_label` that classifies the image. Remember, we want to generate a numerical label, and again, since we have a binary dataset, I’ll create a label that is a 1 if an image is predicted to be day and a 0 for images predicted to be night.\n",
    "\n",
    "I can create a complete classifier by writing a function that takes in an image, extracts the brightness feature, and then checks if the average brightness is above some threshold X.\n",
    "\n",
    "If it is, this classifier returns a 1 (day), and if it’s not, this classifier returns a 0 (night)!\n",
    "\n",
    "Next, you'll take a look at this notebook and get a chance to tweak the threshold parameter. Then, when you're able to generate predicted labels, you can compare them to the true labels, and check the accuracy of our model!\n",
    "\n",
    "### Accuracy\n",
    "The accuracy of a classification model is found by comparing predicted and true labels. For any given image, if the `predicted_label` matches `thetrue_label`, then this is a correctly classified image, if not, it is misclassified.\n",
    "\n",
    "The accuracy is given by the number of correctly classified images divided by the total number of images. We’ll test this classification model on new images, this is called a test set of data.\n",
    "\n",
    "### Test Data\n",
    "Test data is previously unseen image data. The data you have seen, and that you used to help build a classifier is called training data, which we've been referring to. The idea in creating these two sets is to have one set that you can analyze and learn from (training), and one that you can get a sense of how your classifier might work in a real-world, general scenario. You could imagine going through each image in the training set and creating a classifier that can classify all of these training images correctly, but, you actually want to build a classifier that __recognizes general patterns in data__, so that when it is faced with a real-world scenario, it will still work!\n",
    "\n",
    "So, we use a new, test set of data to see how a classification model might work in the real-world and to determine the accuracy of the model.\n",
    "\n",
    "### Misclassified Images\n",
    "In this and most classification examples, there are a few misclassified images in the test set. To see how to improve, it’s useful to take a look at these misclassified images; look at what they were mistakenly labeled as and where your model fails. It will be up to you to look at these images and think about how to improve the classification model!\n",
    "\n",
    "### Review and the Computer Vision Pipeline\n",
    "In this lesson, you’ve really made it through a lot of material, from learning how images are represented to programming an image classifier!\n",
    "\n",
    "You approached the classification challenge by completing each step of the __Computer Vision Pipeline__ ste-by-step. First by looking at the classification problem, visualizing the image data you were working with, and planning out a complete approach to a solution.\n",
    "\n",
    "The steps include __pre-processing__ images so that they could be further analyzed in the same way, this included changing color spaces. Then we moved on to __feature extraction__, in which you decided on distinguishing traits in each class of image, and tried to isolate those features! You may note that skipped the pipeline step of \"Selecting Areas of Interest,\" and this is because we focused on classifying an image as a whole and did not need break it up into different segments, but we'll see where this step can be useful later in this course.\n",
    "\n",
    "Finally, you created a complete __classifier__ that output a label or a class for a given image, and analyzed your classification model to see its accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe9145",
   "metadata": {},
   "source": [
    "## Convolutional Filters and Edge Detection\n",
    "\n",
    "### Filters\n",
    "Now, we’ve seen how to use color to help isolate a desired portion of an image and even help classify an image!\n",
    "\n",
    "In addition to taking advantage of color information, we also have knowledge about patterns of grayscale intensity in an image. Intensity is a measure of light and dark similar to brightness, and we can use this knowledge to detect other areas or objects of interest. For example, you can often identify the edges of an object by looking at an abrupt change in intensity, which happens when an image changes from a very dark to light area, or vice versa.\n",
    "\n",
    "To detect these changes, you’ll be using and creating specific image filters that look at groups of pixels and detect big changes in intensity in an image. These filters produce an output that shows these edges.\n",
    "\n",
    "So, let’s take a closer look at these filters and see when they’re useful in processing images and identifying traits of interest.\n",
    "\n",
    "### Frequency in images\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s ([Hz](https://en.wikipedia.org/wiki/Hertz)), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound and on the x-axis is time.\n",
    "\n",
    "### High and low frequency\n",
    "Similarly, frequency in images is a rate of change. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.\n",
    "\n",
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "__High-frequency components also correspond to the edges of objects in images__, which can help us classify those objects.\n",
    "\n",
    "### Fourier Transform\n",
    "The Fourier Transform (FT) is an important image processing tool which is used to decompose an image into its frequency components. The output of an FT represents the image in the frequency domain, while the input image is the spatial domain (x, y) equivalent. In the frequency domain image, each point represents a particular frequency contained in the spatial domain image. So, for images with a lot of high-frequency components (edges, corners, and stripes), there will be a number of points in the frequency domain at high frequency values.\n",
    "\n",
    "Take a look at how FT's are done with OpenCV, [here](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.html)\n",
    "\n",
    "### Edge Handling\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "__Extend__ The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "__Padding__ The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "__Crop__ Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2d1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
