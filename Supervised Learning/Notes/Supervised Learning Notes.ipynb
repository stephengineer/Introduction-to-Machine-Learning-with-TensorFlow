{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49b08d4",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372fe59",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$$\n",
    "w_i \\rightarrow w_i - \\alpha \\frac{{\\partial}}{{\\partial w_i}} Error\n",
    "$$\n",
    "\n",
    "#### Error Functions\n",
    "- Mean Absolute Error\n",
    "\n",
    "![MeanAbsoluteError](img/MeanAbsoluteError.png)\n",
    "\n",
    "$$\n",
    "Error = \\frac{1}{m} \\sum_{i=1}^m |y- \\hat{y}|\n",
    "$$\n",
    "\n",
    "- Mean Squared Error\n",
    "\n",
    "![MeanSquaredError](img/MeanSquaredError.png)\n",
    "\n",
    "$$\n",
    "Error = \\frac{1}{2m} \\sum_{i=1}^m (y- \\hat{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "### [Mini-batch Gradient Descent](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/Mini-batch%20Gradient%20Descent.pdf)\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    "\n",
    "![batch-stochastic](img/batch-stochastic.png)\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called mini-batch gradient descent.\n",
    "\n",
    "![minibatch](img/minibatch.png)\n",
    "\n",
    "[Quiz: Mini-Batch Gradient Descent](../../edit/01%20Linear%20Regression/batch_graddesc_solution.py)\n",
    "\n",
    "[Programming Quiz: Linear Regression in scikit-learn](../../edit/01%20Linear%20Regression/gapminder1.py)\n",
    "\n",
    "[Programming Quiz: Multiple Linear Regression](../../edit/01%20Linear%20Regression/multiple_linear_Regression.py)\n",
    "\n",
    "\n",
    "### [Linear Regression Warnings](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/Linear%20Regression%20Warnings.pdf)\n",
    "\n",
    "__Linear Regression Works Best When the Data is Linear__\n",
    "Linear regression produces a straight line model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features (we'll come to this next), or use another kind of model.\n",
    "\n",
    "__Linear Regression is Sensitive to Outliers__\n",
    "Linear regression tries to find a 'best fit' line among the training data. If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect.\n",
    "\n",
    "\n",
    "### Polynomial Regression\n",
    "[Quiz: Polynomial Regression](../../edit/01%20Linear%20Regression/poly_reg.py)\n",
    "\n",
    "\n",
    "### Regularization\n",
    "- L1\n",
    "- L2\n",
    "\n",
    "![Regularization](img/Regularization.png)\n",
    "\n",
    "[Quiz: Regularization](../../edit/01%20Linear%20Regression/regularization.py)\n",
    "\n",
    "\n",
    "### [Feature Scaling](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/FeatureScaling.pdf)\n",
    "\n",
    "What is feature scaling? Feature scaling is a way of transforming your data into a common range of values. There are two common scalings:\n",
    "\n",
    "1. Standardizing\n",
    "__Standardizing__ is completed by taking each value of your column, subtracting the mean of the column, and then dividing by the standard deviation of the column.\n",
    "\n",
    "2. Normalizing\n",
    "With __normalizing__, data are scaled between 0 and 1.\n",
    "\n",
    "#### When Should I Use Feature Scaling?\n",
    "In many machine learning algorithms, the result will change depending on the units of your data. This is especially true in two specific cases:\n",
    "\n",
    "1. When your algorithm uses a distance-based metric to predict.\n",
    "2. When you incorporate regularization.\n",
    "\n",
    "#### Regularization\n",
    "When you start introducing regularization, you will again want to scale the features of your model. The penalty on particular coefficients in regularized linear regression techniques depends largely on the scale associated with the features. When one feature is on a small range, say from 0 to 10, and another is on a large range, say from 0 to 1 000 000, applying regularization is going to unfairly punish the feature with the small range. Features with small ranges need to have larger coefficients compared to features with large ranges in order to have the same effect on the outcome of the data. (Think about how `ab = ba` for two numbers `a` and `b`.) Therefore, if regularization could remove one of those two features with the same net increase in error, it would rather remove the small-ranged feature with the large coefficient, since that would reduce the regularization term the most.\n",
    "\n",
    "[Quiz: Feature Scaling](../../edit/01%20Linear%20Regression/feature_scaling.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe5846",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "### [Entropy](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/03%20Decision%20Trees/Multiclass%20Entropy.pdf)\n",
    "The more rigid the set is or the more homogeneous, the less entropy you have, and vice versa.\n",
    "\n",
    "![Entropy](img/Entropy.png)\n",
    "\n",
    "$$\n",
    "Entropy = -\\frac{m}{m+n}log_2(\\frac{m}{m+n})-\\frac{n}{m+n}log_2(\\frac{n}{m+n})\n",
    "$$\n",
    "\n",
    "We can state this in terms of probabilities instead for the number of red balls as $p_1$ and the number of blue balls as $p_2$:\n",
    "\n",
    "$$\n",
    "p_1=\\frac{m}{m+n}\n",
    "\\\\\n",
    "p_2=\\frac{n}{m+n}\n",
    "\\\\\n",
    "Entropy = -p_1log_2(p_1) - p_2log_2(p_2)\n",
    "$$\n",
    "\n",
    "This entropy equation can be extended to the multi-class case, where we have three or more possible values:\n",
    "\n",
    "$$\n",
    "Entropy = -p_1log_2(p_1) - p_2log_2(p_2) - ... - p_nlog_2(p_n) = \\sum_{i=1}^np_ilog_2(p_i)\n",
    "$$\n",
    "\n",
    "The minimum value is still 0, when all elements are of the same value. The maximum value is still achieved when the outcome probabilities are the same, but the upper limit increases with the number of different outcomes. (For example, you can verify the maximum entropy is 2 if there are four different possibilities, each with probability 0.25.)\n",
    "\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "$$\n",
    "IG = Entropy(Parent)-[\\frac{m}{m+n}Entropy(Child_1)+\\frac{n}{m+n}Entropy(Child_2)]\n",
    "$$\n",
    "\n",
    "### [Hyperparameters](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/03%20Decision%20Trees/Hyperparameters.pdf)\n",
    "\n",
    "- Maximum Depth\n",
    "The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. A tree of maximum length `k` can have at most $2^k$\n",
    "\n",
    "- Minimum number of samples to split\n",
    "A node must have at least `min_samples_split` samples in order to be large enough to split. If a node has fewer samples than `min_samples_split` samples, it will not be split, and the splitting process stops.\n",
    "\n",
    "- Minimum number of samples per leaf\n",
    "When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other. This will not take us too far in our process, and would be a waste of resources and time. If we want to avoid this, we can set a minimum for the number of samples we allow on each leaf.\n",
    "\n",
    "[Decision Trees in sklearn](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/03%20Decision%20Trees/Decision%20Trees%20in%20sklearn.pdf)\n",
    "\n",
    "[Quiz:Decision Trees](../../edit/03%20Decision%20Trees/dt.py)\n",
    "\n",
    "[Lab: Titanic Survival Exploration with Decision Trees](../../notebooks/03%20Decision%20Trees/titanic_survival_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789500d",
   "metadata": {},
   "source": [
    "## Navie Bayes\n",
    "\n",
    "![BayesTheorem](img/BayesTheorem.png)\n",
    "\n",
    "[Practice Project: Building a spam classifier](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/04%20Naive%20Bayes/Building%20a%20Spam%20Classifier.pdf)\n",
    "\n",
    "[Lab: Spam Classifier](../../notebooks/04%20Naive%20Bayes/Bayesian_Inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524a7a1",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "### Error Function\n",
    "\n",
    "Error = Classification Error + Margin Error\n",
    "\n",
    "Minimize using gradient descent\n",
    "\n",
    "- Classification Error\n",
    "\n",
    "![ClassificationError](img/ClassificationError.png)\n",
    "\n",
    "- [Margin Error](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/05%20Support%20Vector%20machines/Margin%20Error%20Calculation.pdf)\n",
    "\n",
    "![MarginError](img/MarginError.png)\n",
    "\n",
    "    - large margin, small error\n",
    "    - small margin, large error\n",
    "    \n",
    "![MarginError](img/MarginError2.png)\n",
    "\n",
    "\n",
    "### The C Parameter\n",
    "\n",
    "Error = C * Classification Error + Margin Error\n",
    "\n",
    "- Large C: Focus on classifying points, may have a small margin\n",
    "- Small C: Focus on a large margin, may make classification errors\n",
    "\n",
    "\n",
    "### Kernel\n",
    "\n",
    "- Polynomial Kernel\n",
    "- RBF Kernel\n",
    "\n",
    "[SVMs in sklearn](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/05%20Support%20Vector%20machines/SVMs%20in%20sklearn.pdf)\n",
    "\n",
    "[Quiz: SVM](../../edit/05%20Support%20Vector%20machines/svm.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedb6d8",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "### [Ensembles](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/06%20Ensemble%20Methods/Ensembles.pdf)\n",
    "\n",
    "This whole lesson (on ensembles) is about how we can combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models.\n",
    "\n",
    "Commonly the \"weak\" learners you use are decision trees. In fact the default for most ensemble methods is a decision tree in sklearn. However, you can change this value to any of the models you have seen so far.\n",
    "\n",
    "\n",
    "#### Why Would We Want to Ensemble Learners Together?\n",
    "\n",
    "There are two competing variables in finding a well fitting machine learning model: __Bias__ and __Variance__. It is common in interviews for you to be asked about this topic and how it pertains to different modeling techniques. As a first pass, [the wikipedia is quite useful](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). However, I will give you my perspective and examples:\n",
    "\n",
    "__Bias__: When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "__Variance__: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "\n",
    "By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem.\n",
    "\n",
    "- __High Bias, Low Variance__ models tend to underfit data, as they are not flexible. __Linear models__ fall into this category of models.\n",
    "\n",
    "- __High Variance, Low Bias__ models tend to overfit data, as they are too flexible. __Decision trees__ fall into this category of models.\n",
    "\n",
    "#### Introducing Randomness Into Ensembles\n",
    "\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "1. __Bootstrap the data__ - that is, sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "\n",
    "2. __Subset the features__ - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "In fact, these are the two random components used in the next algorithm you are going to see called __random forests__.\n",
    "\n",
    "\n",
    "These ensemble methods use a combination of techniques you have seen throughout this lesson:\n",
    "\n",
    "- __Bootstrap the data__ passed through a learner (bagging).\n",
    "- __Subset the features__ used for a learner (combined with bagging signifies the two random components of random forests).\n",
    "- __Ensemble learners__ together in a way that allows those that perform best in certain areas to create the largest impact (boosting).\n",
    "\n",
    "### [Techniques](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "You saw a number of ensemble methods in this lesson including:\n",
    "\n",
    "- [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "\n",
    "#### Random Forests\n",
    "\n",
    "Pick up some features randomly and build a decision tree and do for few times, and the ensemble of tree will recommend the result.\n",
    "\n",
    "![RandomForests](img/RandomForests.png)\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "Since data may be huge, in general, we do not want to train many models on the same data. This would be very expensive. Instead, we will just take subsets of it and train a weak learner on each one of these subsets. Then we will figure out how to combine these learners.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "We fit our first learner in order to maximize accuracy or equivalently minimize the number of errors. The second learner needs to fix on the mistakes that this one has made. We will take the misclassified points and make them bigger. In other words, we will punish the model more if it misses these points, so next weak learner needs to focus on these more. Keep going and combine these models.\n",
    "\n",
    "![AdaBoost](img/AdaBoost.png)\n",
    "\n",
    "##### Combining the Models\n",
    "\n",
    "Calculate weight of each model by:\n",
    "\n",
    "$$\n",
    "weight = ln(\\frac{accuracy}{1-accuracy})\n",
    "$$\n",
    "\n",
    "![CombiningModels](img/CombiningModels.png)\n",
    "\n",
    "![CombiningModels](img/CombiningModels2.png)\n",
    "\n",
    "[AdaBoost in sklearn](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/06%20Ensemble%20Methods/AdaBoost%20in%20sklearn.pdf)\n",
    "\n",
    "[Lab: Spam Classifying](../../notebooks/06%20Ensemble%20Methods/Spam_&_Ensembles.ipynb)\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "Additionally, here are some great resources on AdaBoost if you'd like to learn some more!\n",
    "\n",
    "- Here is the original [paper](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/06%20Ensemble%20Methods/IntroToBoosting.pdf) from Freund and Schapire.\n",
    "\n",
    "- A follow-up [paper](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/06%20Ensemble%20Methods/boostingexperiments.pdf) from the same authors regarding several experiments with Adaboost.\n",
    "\n",
    "- A great [tutorial](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/06%20Ensemble%20Methods/explaining-adaboost.pdf) by Schapire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e1c7e",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "![confusion](img/confusion.png)\n",
    "\n",
    "In this image, the blue points are labelled positive, and the red points are labelled negative. Furthermore, the points on top of the line are predicted (guessed) to be positive, and the points below the line are predicted to be negative.\n",
    "\n",
    "#### Type 1 and Type 2 Errors\n",
    "Sometimes in the literature, you'll see False Positives and False Negatives as Type 1 and Type 2 errors. Here is the correspondence:\n",
    "\n",
    "- __Type 1 Error (Error of the first kind, or False Positive)__: In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "- __Type 2 Error (Error of the second kind, or False Negative)__: In the medical example, this is when we misdiagnose a sick patient as healthy.\n",
    "\n",
    "\n",
    "## Classification Measures\n",
    "If you are fitting your model to predict categorical data (spam not spam), there are different measures to understand how well your model is performing than if you are predicting numeric values (the price of a home).\n",
    "\n",
    "As we look at classification metrics, note that the [wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) on this topic is wonderful, but also a bit daunting. I frequently use it to remember which metric does what.\n",
    "\n",
    "Specifically, you saw how to calculate:\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is often used to compare models, as it tells us the proportion of observations we correctly labeled.\n",
    "\n",
    "![accuracy](img/accuracy.png)\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{Correctly Classified Points}{All points} = \\frac{True Positives + True Negatives}{Total}\n",
    "$$\n",
    "\n",
    "Often accuracy is not the only metric you should be optimizing on. This is especially the case when you have class imbalance in your data. Optimizing on only accuracy can be misleading in how well your model is truly performing. With that in mind, you saw some additional metrics.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Out of the points we have predicted to be positive, how many are correct?\n",
    "\n",
    "Precision focuses on the __predicted__ \"positive\" values in your dataset. By optimizing based on precision values, you are determining if you are doing a good job of predicting the positive values, as compared to predicting negative values as positive.\n",
    "\n",
    "![precision](img/precision.png)\n",
    "\n",
    "$$\n",
    "Precision = \\frac{True Positive}{True Positive + False Positive}\n",
    "$$\n",
    "\n",
    "### Recall\n",
    "\n",
    "Out of the points labelled \"positive\", how many did we correctly predict?\n",
    "\n",
    "Recall focuses on the __actual__ \"positive\" values in your dataset. By optimizing based on recall values, you are determining if you are doing a good job of predicting the positive values __without__ regard of how you are doing on the __actual__ negative values. If you want to perform something similar to recall on the __actual__ 'negative' values, this is called specificity (TN / (TN + FP)).\n",
    "\n",
    "![recall](img/recall.png)\n",
    "\n",
    "$$\n",
    "Recall = \\frac{True Positive}{True Positive + True Negative}\n",
    "$$\n",
    "\n",
    "![PrecisionRecall](img/PrecisionRecall.png)\n",
    "\n",
    "\n",
    "### F-beta Score\n",
    "\n",
    "In order to look at a combination of metrics at the same time, there are some common techniques like the F-Beta Score (where the F1 score is frequently used), as well as the ROC and AUC. You can see that the $\\beta$ parameter controls the degree to which precision is weighed into the F score, which allows precision and recall to be considered simultaneously. The most common value for beta is 1, as this is where you are finding the harmonic average between precision and recall.\n",
    "\n",
    "$$\n",
    "F_{\\beta} Score = (1+\\beta^2) \\cdot \\frac{Precision \\cdot Recall}{(\\beta^2 \\cdot Precision) + Recall}\n",
    "$$\n",
    "\n",
    "- If $\\beta$ = 0, then we get precision.\n",
    "- If $\\beta$ = $\\infty$, then we get recall.\n",
    "- For other values of $\\beta$, if they are close to 0, we get something close to precision, if they are large numbers, then we get something close to recall, and if $\\beta$ = 1, then we get the __harmonic mean__ of precision and recall.\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "$$\n",
    "F1 Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "### ROC Curve & AUC\n",
    "\n",
    "Receiver Operating Characteristic\n",
    "\n",
    "By finding different thresholds for our classification metrics, we can measure the area under the curve (where the curve is known as a ROC curve). Similar to each of the other metrics above, when the AUC is higher (closer to 1), this suggests that our model performance is better than when our metric is close to 0.\n",
    "\n",
    "$$\n",
    "True Positive Rate = \\frac{True Positives}{All Positives}\n",
    "\\\\\n",
    "False Positive Rate = \\frac{False Positives}{All Positives}\n",
    "$$\n",
    "\n",
    "![ROCCurve](img/ROCCurve.png)\n",
    "\n",
    "![ROC](img/ROC.png)\n",
    "\n",
    "__Summary__: The closer your area under the ROC curve is to 1, the better your model is.\n",
    "\n",
    "You may end up choosing to optimize on any of these measures. I commonly end up using AUC or an F1 score in practice. However, there are always reason to choose one measure over another depending on your situation.\n",
    "\n",
    "\n",
    "## Regression Measures\n",
    "You want to measure how well your algorithms are performing on predicting numeric values? In these cases, there are three main metrics that are frequently used. __mean absolute error__, __mean squared error__, and __r2__ values.\n",
    "\n",
    "As an important note, optimizing on the mean absolute error may lead to a different 'best model' than if you optimize on the mean squared error. However, optimizing on the mean squared error will __always__ lead to the same 'best' model as if you were to optimize on the __r2__ value.\n",
    "\n",
    "Again, if you choose a model with the best r2 value (the highest), it will also be the model that has the lowest (MSE). Choosing one versus another is based on which one you feel most comfortable explaining to someone else.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "The first metric you saw was the mean absolute error. This is a useful metric to optimize on when the value you are trying to predict follows a skewed distribution. Optimizing on an absolute value is particularly helpful in these cases because outliers will not influence models attempting to optimize on this metric as much as if you use the mean squared error. The optimal value for this technique is the median value. When you optimize for the R2 value of the mean squared error, the optimal value is actually the mean.\n",
    "\n",
    "![mae](img/mae.png)\n",
    "\n",
    "### Mean-Squared Error (MSE)\n",
    "\n",
    "The mean squared error is by far the most used metric for optimization in regression problems. Similar to with MAE, you want to find a model that minimizes this value. This metric can be greatly impacted by skewed distributions and outliers. When a model is considered optimal via MAE, but not for MSE, it is useful to keep this in mind. In many cases, it is easier to actually optimize on MSE, as the a quadratic term is differentiable. However, an absolute value is not differentiable. This factor makes this metric better for gradient based optimization algorithms.\n",
    "\n",
    "![mse](img/mse.png)\n",
    "\n",
    "### R2 Score\n",
    "\n",
    "Finally, the r2 value is another common metric when looking at regression values. Optimizing a model to have the lowest MSE will also optimize a model to have the the highest R2 value. This is a convenient feature of this metric. The R2 value is frequently interpreted as the 'amount of variability' captured by a model. Therefore, you can think of MSE, as the average amount you miss by across all the points, and the R2 value as the amount of the variability in the points that you capture with a model.\n",
    "\n",
    "![r2](img/r2.png)\n",
    "\n",
    "\n",
    "## [Recap](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/07%20Model%20Evaluation%20Metrics/Recap.pdf)\n",
    "\n",
    "[Lab: Classification_Metrics](../../notebooks/07%20Model%20Evaluation%20Metrics/Classification_Metrics.ipynb)\n",
    "\n",
    "[Lab: Regression Metrics](../../notebooks/07%20Model%20Evaluation%20Metrics/Regression%20Metrics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a1a24",
   "metadata": {},
   "source": [
    "## Training and Tuning\n",
    "\n",
    "### Types of Errors\n",
    "\n",
    "![tradeoff](img/tradeoff.png)\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "- Training dataset: training our model\n",
    "- Cross Validation: making decisions\n",
    "- Testing dataset: final testing\n",
    "\n",
    "![CrossValidation](img/CrossValidation.png)\n",
    "\n",
    "### K-Fold Cross Validation\n",
    "\n",
    "![K-FoldCrossValidation](img/K-FoldCrossValidation.png)\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "![LearningCurves](img/learning-curves.png)\n",
    "\n",
    "[Detecting Overfitting and Underfitting](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/08%20Training%20and%20Tuning/Detecting%20Overfitting%20and%20Underfitting%20Solution.pdf)\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "[Grid Search in sklearn](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/08%20Training%20and%20Tuning/Grid%20Search%20in%20sklearn.pdf)\n",
    "\n",
    "[Lab: Grid Search](../../notebooks/08%20Training%20and%20Tuning/Grid_Search_Lab.ipynb)\n",
    "\n",
    "[Lab: Diabetes Case Study](../../notebooks/08%20Training%20and%20Tuning/Diabetes%20Case%20Study.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee1c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
