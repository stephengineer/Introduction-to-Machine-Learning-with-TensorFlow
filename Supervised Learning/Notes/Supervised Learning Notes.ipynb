{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1821e1e",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372fe59",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$$\n",
    "w_i \\rightarrow w_i - \\alpha \\frac{{\\partial}}{{\\partial w_i}} Error\n",
    "$$\n",
    "\n",
    "#### Error Functions\n",
    "- Mean Absolute Error\n",
    "\n",
    "![MeanAbsoluteError](img/MeanAbsoluteError.png)\n",
    "\n",
    "$$\n",
    "Error = \\frac{1}{m} \\sum_{i=1}^m |y- \\hat{y}|\n",
    "$$\n",
    "\n",
    "- Mean Squared Error\n",
    "\n",
    "![MeanSquaredError](img/MeanSquaredError.png)\n",
    "\n",
    "$$\n",
    "Error = \\frac{1}{2m} \\sum_{i=1}^m (y- \\hat{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "### [Mini-batch Gradient Descent](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/Mini-batch%20Gradient%20Descent.pdf) \n",
    "#### Batch Gradient Descent\n",
    "By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    "\n",
    "![batch-stochastic](img/batch-stochastic.png)\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called mini-batch gradient descent.\n",
    "\n",
    "![minibatch](img/minibatch.png)\n",
    "\n",
    "[Quiz: Mini-Batch Gradient Descent](../../edit/01%20Linear%20Regression/batch_graddesc_solution.py)\n",
    "\n",
    "[Programming Quiz: Linear Regression in scikit-learn](../../edit/01%20Linear%20Regression/gapminder1.py)\n",
    "\n",
    "[Programming Quiz: Multiple Linear Regression](../../edit/01%20Linear%20Regression/multiple_linear_Regression.py)\n",
    "\n",
    "\n",
    "### [Linear Regression Warnings](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/Linear%20Regression%20Warnings.pdf)\n",
    "\n",
    "__Linear Regression Works Best When the Data is Linear__\n",
    "Linear regression produces a straight line model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features (we'll come to this next), or use another kind of model.\n",
    "\n",
    "__Linear Regression is Sensitive to Outliers__\n",
    "Linear regression tries to find a 'best fit' line among the training data. If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect.\n",
    "\n",
    "\n",
    "### Polynomial Regression\n",
    "[Quiz: Polynomial Regression](../../edit/01%20Linear%20Regression/poly_reg.py)\n",
    "\n",
    "\n",
    "### Regularization\n",
    "- L1\n",
    "- L2\n",
    "\n",
    "![Regularization](img/Regularization.png)\n",
    "\n",
    "[Quiz: Regularization](../../edit/01%20Linear%20Regression/regularization.py)\n",
    "\n",
    "\n",
    "### [Feature Scaling](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Supervised%20Learning/01%20Linear%20Regression/FeatureScaling.pdf)\n",
    "\n",
    "What is feature scaling? Feature scaling is a way of transforming your data into a common range of values. There are two common scalings:\n",
    "\n",
    "1. Standardizing\n",
    "__Standardizing__ is completed by taking each value of your column, subtracting the mean of the column, and then dividing by the standard deviation of the column.\n",
    "\n",
    "2. Normalizing\n",
    "With __normalizing__, data are scaled between 0 and 1.\n",
    "\n",
    "#### When Should I Use Feature Scaling?\n",
    "In many machine learning algorithms, the result will change depending on the units of your data. This is especially true in two specific cases:\n",
    "\n",
    "1. When your algorithm uses a distance-based metric to predict.\n",
    "2. When you incorporate regularization.\n",
    "\n",
    "\n",
    "#### Regularization\n",
    "When you start introducing regularization, you will again want to scale the features of your model. The penalty on particular coefficients in regularized linear regression techniques depends largely on the scale associated with the features. When one feature is on a small range, say from 0 to 10, and another is on a large range, say from 0 to 1 000 000, applying regularization is going to unfairly punish the feature with the small range. Features with small ranges need to have larger coefficients compared to features with large ranges in order to have the same effect on the outcome of the data. (Think about how `ab = ba` for two numbers `a` and `b`.) Therefore, if regularization could remove one of those two features with the same net increase in error, it would rather remove the small-ranged feature with the large coefficient, since that would reduce the regularization term the most.\n",
    "\n",
    "[Quiz: Feature Scaling](../../edit/01%20Linear%20Regression/feature_scaling.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa912daf",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d0631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
