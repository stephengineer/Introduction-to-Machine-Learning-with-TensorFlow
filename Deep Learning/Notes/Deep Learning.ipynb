{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd884ce9",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Github Repos\n",
    "\n",
    "- [Deep Learning](https://github.com/udacity/deep-learning)\n",
    "- [Deep Learning with Pytorch](https://github.com/udacity/deep-learning-v2-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da5e67",
   "metadata": {},
   "source": [
    "## Introductioin to Neural Networks\n",
    "\n",
    "### Gradient Descent\n",
    "[Principles and the math behind the gradient descent algorithm](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/Introduction%20to%20Neural%20Networks/Gradient%20Descent.pdf)\n",
    "\n",
    "#### Error Function\n",
    "\n",
    "- The error function should be differentiable\n",
    "- THe error function should be continuous\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Derivative of the sigmoid function\n",
    "$$\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$\n",
    "\n",
    "\n",
    "```python\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
    "\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = y - output\n",
    "    weights += learnrate * d_error * x\n",
    "    bias += learnrate * d_error\n",
    "    return weights, bias\n",
    "```\n",
    "\n",
    "### One-hot Encoding\n",
    "Use the `get_dummies` function in Pandas in order to one-hot encode the data.\n",
    "\n",
    "```python\n",
    "# Make dummy variables for rank\n",
    "one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n",
    "```\n",
    "\n",
    "### Maximum Likelihood\n",
    "- log(ab) = log(a) + log(b)\n",
    "\n",
    "### Cross Entropy\n",
    "A higher cross-entropy implies a lower probability for an event. (cross-entropy is inversely proportional to the total probability of an outcome.)\n",
    "\n",
    "- A good model gives a low cross entropy\n",
    "- A bad model gives a high cross entropy\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{i=1}^m y_i ln(p_i) + (1-y_i) ln (1-p_i)\n",
    "$$\n",
    "\n",
    "#### Coding Cross-entropy\n",
    "```python\n",
    "# Y is for the category, and P is the probability.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "```\n",
    "\n",
    "### Logistic Regression\n",
    "1. Start with random weights: $w_1, ... , w_n, b$\n",
    "2. For every point $(x_1, ... , x_n)$: update $w_i, b$\n",
    "3. Reapeat until error is small\n",
    "\n",
    "### Neural Network Architecture\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "### Backpropagation\n",
    "- Doing a feedforward operation.\n",
    "- Comparing the output of the model with the desired output.\n",
    "- Calculating the error.\n",
    "- Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model.\n",
    "- Continue this until we have a model that is good.\n",
    "\n",
    "#### Backpropagate the error\n",
    "$$ (y-\\hat{y}) \\sigma'(x) $$\n",
    "\n",
    "```python\n",
    "def error_term_formula(x, y, output):\n",
    "    return (y - output)*sigmoid_prime(x)\n",
    "```\n",
    "\n",
    "[Lab: Analyzing Student Data](../../notebooks/01%20Introduction%20to%20Neural%20Networks/StudentAdmissions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74087f",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "\n",
    "### Mean Squared Error Function\n",
    "$$\n",
    "E=\\frac{1}{2m}\\sum_{\\mu}(y^{\\mu}-\\hat{y}^{\\mu})^2\n",
    "$$\n",
    "\n",
    "- [Gradient Descent](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent.pdf)\n",
    "- [Gradient Descent Code](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Code.pdf)\n",
    "- [Gradient Descent Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Implementing.pdf)\n",
    "- [Multilayer Perceptrons](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Multilayer%20Perceptrons.pdf)\n",
    "- [Backpropagation](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation.pdf)\n",
    "- [Backpropagation Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation%20Implementing.pdf)\n",
    "\n",
    "Further reading\n",
    "- From Andrej Karpathy: [Yes, you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)\n",
    "- Also from Andrej Karpathy, [a lecture from Stanford's CS231n course](https://www.youtube.com/watch?v=59Hbtz7XgjM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6672e0",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "- Overfitting -> high variance\n",
    "- Underfitting -> high bias\n",
    "\n",
    "![earlyStopping](./img/earlyStopping.png)\n",
    "\n",
    "### Regularization\n",
    "Large coefficients -> overfitting\n",
    "- L1 Error Function: Good for feature selection\n",
    "$$= -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(|w_1|+...+|w_n|)$$\n",
    "- L2 Error Function: Normally better for training models\n",
    "$$E = -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(w_1^2+...+w_n^2)$$\n",
    "\n",
    "### Dropout\n",
    "Prevent overfitting\n",
    "\n",
    "### Random Restart\n",
    "Jump out the local minima\n",
    "\n",
    "### Vanishing Gradient\n",
    "- Hyperbolic tangent function\n",
    "$$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "- Rectified Linear Unit (ReLU)\n",
    "$$\n",
    "relu(x)=\n",
    "\\begin{cases}\n",
    "x & if x\\ge 0\\\\\n",
    "0 & if x<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent\n",
    "Decrease training time\n",
    "\n",
    "### Learning Rate Decay\n",
    "Rule:\n",
    "- If steep: long steps\n",
    "- If plain: small steps\n",
    "\n",
    "### Momentum\n",
    "Solve local minmum problem.\n",
    "- STEP: average of previous steps\n",
    "- $\\beta$: momentum\n",
    "- STEP(n) $\\rightarrow$ STEP(n) + $\\beta$ STEP(n-1) + $\\beta^2$ STEP(n-2) + ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37eba8",
   "metadata": {},
   "source": [
    "## [Deep Learning with TensorFlow](https://github.com/udacity/intro-to-ml-tensorflow)\n",
    "\n",
    "### Build Neural Network\n",
    "[Part 1 Introduction to Neural Networks with TensorFlow](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_1_Introduction_to_Neural_Networks_with_TensorFlow_(Solution).ipynb)\n",
    "\n",
    "[Part 2 Neural networks with TensorFlow and Keras](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_2_Neural_networks_with_TensorFlow_and_Keras_(Solution).ipynb)\n",
    "\n",
    "- `tf.multiply()`: Performs element-wise multiplication on two inputs\n",
    "- `tf.matmul()`: Performs matrix multiplication on two inputs\n",
    "- `tf.reduce_sum()`: Computes the sum of elements across an input tensor's dimensions\n",
    "- `tf.convert_to_tensor()`: convert ndarray to a TensorFlow tensor\n",
    "- `tensor.numpy()`: command on the tensor itself to convert it to an ndarray\n",
    "\n",
    "There are [plenty of different datasets](https://www.tensorflow.org/datasets/catalog/overview) available from the `tensorflow_datasets` library, which we shortened in the code to `tfds`. Loading one of the datasets is simple with the `tfds.load()` function, which takes in the dataset name (in this case `mnist`), as well as some other optional arguments such as: 1) the dataset split to get (training, test, validation), 2) whether to shuffle the data, 3) if the data is to be used as part of a supervised learning algorithm (including labels), 4) whether to include metadata about the dataset itself, and [more](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
    "\n",
    "You can use the `.take()` function with an integer as an argument to get a certain number of images at once from the dataset.\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "- [Pipeline Performance](https://www.tensorflow.org/guide/data_performance)\n",
    "- [Transformations](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "To calculate this probability distribution, we often use the [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Mathematically this looks like\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "TensorFlow also includes one of its own built-in Softmax activation functions you can use. Using the [TensorFlow API documentation]\n",
    "\n",
    "- `tf.nn.softmax`\n",
    "- `tf.math.softmax`\n",
    "- `tf.keras.activations.softmax`\n",
    "\n",
    "#### Neural Networks with TensorFlow\n",
    "\n",
    "Keras helps further simplify working with neural networks running on TensorFlow under the hood. You can more easily stack layers with `tf.keras.Sequential`, making sure to feed an `input_shape` to the first layer of the network. You can also either add separate `Activation` layers, or feed an activation as an argument within certain layers, such as the `Dense` fully-connected layers.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(256, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "#### Subclassing\n",
    "```python\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "        # Define layers \n",
    "        self.input_layer = tf.keras.layers.Flatten()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(256, activation = 'relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(self.num_classes, activation = 'softmax')\n",
    "    \n",
    "    # Define forward Pass   \n",
    "    def call(self, input_tensor):\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a model object\n",
    "subclassed_model = Network(10)\n",
    "\n",
    "# Build the model, i.e. initialize the model's weights and biases\n",
    "subclassed_model.build((None, 28, 28, 1))\n",
    "\n",
    "subclassed_model.summary()\n",
    "```\n",
    "\n",
    "#### Adding Layers with .add\n",
    "\n",
    "Example:\n",
    "```python\n",
    "layer_neurons = [512, 256, 128, 56, 28, 14]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "            \n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "          \n",
    "model.summary() \n",
    "```\n",
    "\n",
    "#### Clearing the Graph\n",
    "\n",
    "In order to avoid clutter from old models in the graph, we can use:\n",
    "\n",
    "```python\n",
    "tf.keras.backend.clear_session()\n",
    "```\n",
    "\n",
    "This command deletes the current `tf.keras` graph and creates a new one.\n",
    "\n",
    "\n",
    "### Train Neural Network\n",
    "[Part 3 Training Neural Networks](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_3_Training_Neural_Networks_(Solution).ipynb)\n",
    "\n",
    "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
    "\n",
    "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "These are the main parameters we are going to set throught these lesson. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "#### Training the Model\n",
    "\n",
    "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:\n",
    "\n",
    "```python\n",
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)\n",
    "```\n",
    "\n",
    "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson. \n",
    "\n",
    "With our model trained, we can check out it's predictions.\n",
    "\n",
    "```python\n",
    "## Build model\n",
    "my_model = tf.keras.Sequential([\n",
    "           tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "           tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "\n",
    "my_model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "## Train model\n",
    "EPOCHS = 5\n",
    "\n",
    "history = my_model.fit(training_batches, epochs = EPOCHS)\n",
    "\n",
    "\n",
    "## Predict model\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = my_model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "```\n",
    "\n",
    "\n",
    "### Train Neural Network on Complex Dataset\n",
    "[Part 4 Fashion MNIST](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_4_Fashion_MNIST_(Solution).ipynb)\n",
    "\n",
    "[Part 5 Inference and Validation](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_5_Inference_and_Validation_(Solution).ipynb)\n",
    "\n",
    "### Inference & Validation\n",
    "\n",
    "We used `tfds.Split.ALL.subsplit` to make a 60/20/20 split for training, validation and test sets, although some TensorFlow datasets have these subsections already built in. Depending on the dataset, you may also want to make sure to shuffle the data at this point as well.\n",
    "\n",
    "Avoid overfitting to the training data?\n",
    "- Stop training when the training and validation curves start to diverge by a certain amount\n",
    "- Save down the best validation accuracy model from during training\n",
    "- Add layers like Dropout to help generalize the network\n",
    "\n",
    "\n",
    "### Saving & Loading\n",
    "[Part 6 Saving and Loading Models](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_6_Saving_and_Loading_Models.ipynb)\n",
    "\n",
    "In TensorFlow we can save our trained models in different formats. Here we will see how to save our models in TensorFlow's SavedModel format and as HDF5 files, which is the format used by Keras models.\n",
    "\n",
    "#### Saving and Loading Models in HDF5 Format\n",
    "\n",
    "To save our models in the format used by Keras models we use the `.save(filepath)` method. For example, to save a model called `my_model` in the current working directory with the name `test_model` we use:\n",
    "\n",
    "```python\n",
    "my_model.save('./test_model.h5')\n",
    "```\n",
    "\n",
    "It's important to note that we have to provide the `.h5` extension to the `filepath` in order the tell `tf.keras` to save our model as an HDF5 file. \n",
    "\n",
    "The above command saves our model into a single HDF5 file that will contain:\n",
    "\n",
    "* The model's architecture.\n",
    "* The model's weight values which were learned during training.\n",
    "* The model's training configuration, which corresponds to the parameters you passed to the `compile` method.\n",
    "* The optimizer and its state. This allows you to resume training exactly where you left off.\n",
    "\n",
    "\n",
    "In the cell below we save our trained `model` as an HDF5 file. The name of our HDF5 will correspond to the current time stamp. This is useful if you are saving many models and want each of them to have a unique name. By default the `.save()` method will **silently** overwrite any existing file at the target location with the same name. If we want `tf.keras` to provide us with a manual prompt to whether overwrite files with the same name, you can set the argument `overwrite=False` in the `.save()` method.\n",
    "\n",
    "```python\n",
    "t = time.time()\n",
    "\n",
    "saved_keras_model_filepath = './{}.h5'.format(int(t))\n",
    "\n",
    "model.save(saved_keras_model_filepath)\n",
    "```\n",
    "\n",
    "Once a model has been saved, we can use `tf.keras.models.load_model(filepath)` to re-load our model. This command will also compile our model automatically using the saved training configuration, unless the model was never compiled in the first place.\n",
    "\n",
    "```python\n",
    "reloaded_keras_model = tf.keras.models.load_model(saved_keras_model_filepath)\n",
    "```\n",
    "\n",
    "#### Saving and Loading TensorFlow SavedModels\n",
    "\n",
    "To export our models to the TensorFlow **SavedModel** format, we use the `tf.saved_model.save(model, export_dir)` function. For example, to save a model called `my_model` in a folder called `saved_models` located in the current working directory we use:\n",
    "\n",
    "```python\n",
    "tf.saved_model.save(my_model, './saved_models')\n",
    "```\n",
    "\n",
    "It's important to note that here we have to provide the path to the directory where we want to save our model, **NOT** the name of the file. This is because SavedModels are not saved in a single file. Rather, when you save your model as a SavedModel, `the tf.saved_model.save()` function will create an `assets` folder, a `variables` folder, and a `saved_model.pb` file inside the directory you provided.\n",
    "\n",
    "The SavedModel files that are created contain:\n",
    "\n",
    "* A TensorFlow checkpoint containing the model weights.\n",
    "* A SavedModel proto containing the underlying TensorFlow graph. Separate graphs are saved for prediction (serving), training, and evaluation. If the model wasn't compiled before, then only the inference graph gets exported.\n",
    "* The model's architecture configuration if available.\n",
    "\n",
    "The SavedModel is a standalone serialization format for TensorFlow objects, supported by TensorFlow serving as well as TensorFlow implementations other than Python. It does not require the original model building code to run, which makes it useful for sharing or deploying in different platforms, such as mobile and embedded devices (with TensorFlow Lite), servers (with TensorFlow Serving), and even web browsers (with TensorFlow.js).\n",
    "\n",
    "In the cell below we save our trained model as a SavedModel. The name of the folder where we are going to save our model will correspond to the current time stamp. Again, this is useful if you are saving many models and want each of them to be saved in a unique directory.\n",
    "\n",
    "```python\n",
    "t = time.time()\n",
    "\n",
    "savedModel_directory = './{}'.format(int(t))\n",
    "\n",
    "tf.saved_model.save(model, savedModel_directory)\n",
    "```\n",
    "\n",
    "Once a model has been saved as a SavedModel, we can use `tf.saved_model.load(export_dir)` to re-load our model. \n",
    "\n",
    "```python\n",
    "reloaded_SavedModel = tf.saved_model.load(savedModel_directory)\n",
    "```\n",
    "\n",
    "It's important to note that the object returned by `tf.saved_model.load` is **NOT** a Keras object. Therefore, it doesn't have `.fit`, `.predict`, `.summary`, etc. methods. It is 100% independent of the code that created it. This means that in order to make predictions with our `reloaded_SavedModel` we need to use a different method than the one used with the re-loaded Keras model.\n",
    "\n",
    "To make predictions on a batch of images with a re-loaded SavedModel we have to use:\n",
    "\n",
    "```python\n",
    "reloaded_SavedModel(image_batch, training=False)\n",
    "```\n",
    "\n",
    "This will return a tensor with the predicted label probabilities for each image in the batch. Again, since we haven't done anything new to this re-loaded SavedModel, then both the `reloaded_SavedModel` and our original `model` should be identical copies. Therefore, they should make the same predictions on the same images.\n",
    "\n",
    "We can also get back a full Keras model, from a TensorFlow SavedModel, by loading our SavedModel with the `tf.keras.models.load_model` function. \n",
    "\n",
    "```python\n",
    "reloaded_keras_model_from_SavedModel = tf.keras.models.load_model(savedModel_directory)\n",
    "```\n",
    "\n",
    "#### Saving Models During Training\n",
    "\n",
    "We have seen that when we train a model with a validation set, the value of the validation loss changes through the training process. Since the value of the validation loss is an indicator of how well our model will generalize to new data, it will be great if could save our model at each step of the training process and then only keep the version with the lowest validation loss. \n",
    "\n",
    "We can do this in `tf.keras` by using the following callback:\n",
    "\n",
    "```python\n",
    "tf.keras.callbacks.ModelCheckpoint('./best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "```\n",
    "This callback will save the model as a Keras HDF5 file after every epoch. With the `save_best_only=True` argument, this callback will first check the validation loss of the latest model against the one previously saved. The callback will only save the latest model and overwrite the old one, if the latest model has a lower validation loss than the one previously saved. This will guarantee that will end up with the version of the model that achieved the lowest validation loss during training.\n",
    "\n",
    "### Loading Images with TensorFlow\n",
    "[Part 7 Loading Image Data](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_7_Loading_Image_Data_(Solution).ipynb)\n",
    "\n",
    "### Data Augmentation\n",
    "`tf.keras` offers many other transformations that we can apply to our images. You can take a look at all the available transformations in the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#arguments)\n",
    "\n",
    "* rotation_range\n",
    "* width_shift_range\n",
    "* height_shift_range\n",
    "* shear_range\n",
    "* zoom_range\n",
    "* horizontal_flip\n",
    "* fill_mode\n",
    "\n",
    "### Creating a Validation Data Generator\n",
    "Generally, we only apply data augmentation to our training data. Therefore, for the validation set we only need to normalize the pixel values of our images.\n",
    "\n",
    "### Pre-Notebooks with GPU\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "[Transfer Learning](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Transfer%20Learning.pdf)\n",
    "\n",
    "[Part 8 Transfer Learning](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_8_Transfer_Learning_(Solution).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542be54",
   "metadata": {},
   "source": [
    "## Deep Learning with PyTorch\n",
    "\n",
    "Calculate the output of single layer network using `torch.sum()` or `.sum()` and __matrix multiplication__.\n",
    "\n",
    "### Watch those shapes\n",
    "In general, you'll want to check that the tensors going through your model and other code are the correct shapes. Make use of the `.shape` method during debugging and development.\n",
    "\n",
    "A few things to check if your network isn't training appropriately\n",
    "Make sure you're clearing the gradients in the training loop with `optimizer.zero_grad()`. If you're doing a validation loop, be sure to set the network to evaluation mode with `model.eval()`, then back to training mode with `model.train()`.\n",
    "\n",
    "\n",
    "### CUDA errors\n",
    "Sometimes you'll see this error:\n",
    "\n",
    "```\n",
    "RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #1 ‘mat1’\n",
    "```\n",
    "\n",
    "You'll notice the second type is `torch.cuda.FloatTensor`, this means it's a tensor that has been moved to the GPU. It's expecting a tensor with type `torch.FloatTensor`, no `.cuda` there, which means the tensor should be on the CPU. PyTorch can only perform operations on tensors that are on the same device, so either both CPU or both GPU. If you're trying to run your network on the GPU, check to make sure you've moved the model and all necessary tensors to the GPU with `.to(device)` where `device` is either `\"cuda\"` or `\"cpu\"`.\n",
    "\n",
    "\n",
    "[Tutorial: Deep Learning in PyTorch](http://iamtrask.github.io/2017/01/15/pytorch-tutorial/)\n",
    "\n",
    "[Notebooks](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/05%20Deep%20Learning%20with%20PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d03c",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Project folder](https://github.com/stephengineer/Deep-Learning/tree/main/2_Neural_Networks/Sentiment_Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb964a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
