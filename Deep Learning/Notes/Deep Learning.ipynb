{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70642e5",
   "metadata": {},
   "source": [
    "# [Deep Learning](https://github.com/udacity/deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c85a3",
   "metadata": {},
   "source": [
    "## Introductioin to Neural Networks\n",
    "\n",
    "### Gradient Descent\n",
    "[Principles and the math behind the gradient descent algorithm](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/Introduction%20to%20Neural%20Networks/Gradient%20Descent.pdf)\n",
    "\n",
    "#### Error Function\n",
    "\n",
    "- The error function should be differentiable\n",
    "- THe error function should be continuous\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "- Sigmoid\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "derivative of the sigmoid function\n",
    "$$\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "### One-hot Encoding\n",
    "\n",
    "### Maximum Likelihood\n",
    "- log(ab) = log(a) + log(b)\n",
    "\n",
    "### Cross Entropy\n",
    "A higher cross-entropy implies a lower probability for an event.\n",
    "\n",
    "- A good model gives a low cross entropy\n",
    "- A bad model gives a high cross entropy\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{i=1}^m y_i ln(p_i) + (1-y_i) ln (1-p_i)\n",
    "$$\n",
    "\n",
    "### Logistic Regression\n",
    "1. Start with random weights: $w_1, ... , w_n, b$\n",
    "2. For every point $(x_1, ... , x_n)$: update $w_i, b$\n",
    "3. Reapeat until error is small\n",
    "\n",
    "### Neural Network Architecture\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "### Backpropagation\n",
    "- Doing a feedforward operation.\n",
    "- Comparing the output of the model with the desired output.\n",
    "- Calculating the error.\n",
    "- Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model.\n",
    "- Continue this until we have a model that is good.\n",
    "\n",
    "[Lab: Analyzing Student Data](../../notebooks/01%20Introduction%20to%20Neural%20Networks/StudentAdmissions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516da63",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "\n",
    "### Mean Squared Error Function\n",
    "$$\n",
    "E=\\frac{1}{2m}\\sum_{\\mu}(y^{\\mu}-\\hat{y}^{\\mu})^2\n",
    "$$\n",
    "\n",
    "- [Gradient Descent](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent.pdf)\n",
    "- [Gradient Descent Code](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Code.pdf)\n",
    "- [Gradient Descent Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Implementing.pdf)\n",
    "- [Multilayer Perceptrons](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Multilayer%20Perceptrons.pdf)\n",
    "- [Backpropagation](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation.pdf)\n",
    "- [Backpropagation Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation%20Implementing.pdf)\n",
    "\n",
    "Further reading\n",
    "- From Andrej Karpathy: [Yes, you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)\n",
    "- Also from Andrej Karpathy, [a lecture from Stanford's CS231n course](https://www.youtube.com/watch?v=59Hbtz7XgjM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2c283",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "### Regularization\n",
    "Large coefficients -> overfitting\n",
    "- L1 Error Function: Good for feature selection\n",
    "$$= -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(|w_1|+...+|w_n|)$$\n",
    "- L2 Error Function: Normally better for training models\n",
    "$$E = -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(w_1^2+...+w_n^2)$$\n",
    "\n",
    "### Dropout\n",
    "Prevent overfitting\n",
    "\n",
    "### Random Restart\n",
    "Jump out the local minima\n",
    "\n",
    "### Vanishing Gradient\n",
    "- Hyperbolic tangent function\n",
    "$$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "- Rectified Linear Unit (ReLU)\n",
    "$$\n",
    "relu(x)=\n",
    "\\begin{cases}\n",
    "x & if x\\ge 0\\\\\n",
    "0 & if x<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent\n",
    "Decrease training time\n",
    "\n",
    "### Learning Rate Decay\n",
    "Rule:\n",
    "- If steep: long steps\n",
    "- If plain: small steps\n",
    "\n",
    "### Momentum\n",
    "Solve local minmum problem.\n",
    "- STEP: average of previous steps\n",
    "- $\\beta$: momentum\n",
    "- STEP(n) $\\rightarrow$ STEP(n) + $\\beta$ STEP(n-1) + $\\beta^2$ STEP(n-2) + ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d84a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
