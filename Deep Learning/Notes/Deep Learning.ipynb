{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd884ce9",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Github Repos\n",
    "\n",
    "- [Deep Learning](https://github.com/udacity/deep-learning)\n",
    "- [Deep Learning with Pytorch](https://github.com/udacity/deep-learning-v2-pytorch)\n",
    "\n",
    "## Content\n",
    "\n",
    "1. [Introductioin to Neural Networks](#Introductioin-to-Neural-Networks)\n",
    "2. [Implementing Gradient Descent](#Implementing-Gradient-Descent)\n",
    "3. [Training Neural Network](#Training-Neural-Network)\n",
    "4. [Deep Learning with TensorFlow](#Deep-Learning-with-TensorFlow)\n",
    "5. [Deep Learning with PyTorch](#Deep-Learning-with-PyTorch)\n",
    "6. [Convolutional Neural Networks](#Convolutional-Neural-Networks)\n",
    "7. [Recurrent Neural Networks](#Recurrent-Neural-Networks)\n",
    "8. [Generative Adversarial Networks](#Generative-Adversarial-Networks)\n",
    "9. [Deploying a Model](#Deploying-a-Model)\n",
    "10. [Projects](#Projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da5e67",
   "metadata": {},
   "source": [
    "## Introductioin to Neural Networks\n",
    "\n",
    "### Gradient Descent\n",
    "[Principles and the math behind the gradient descent algorithm](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/Introduction%20to%20Neural%20Networks/Gradient%20Descent.pdf)\n",
    "\n",
    "#### Error Function\n",
    "\n",
    "- The error function should be differentiable\n",
    "- THe error function should be continuous\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Derivative of the sigmoid function\n",
    "$$\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$\n",
    "\n",
    "\n",
    "```python\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
    "\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = y - output\n",
    "    weights += learnrate * d_error * x\n",
    "    bias += learnrate * d_error\n",
    "    return weights, bias\n",
    "```\n",
    "\n",
    "### One-hot Encoding\n",
    "Use the `get_dummies` function in Pandas in order to one-hot encode the data.\n",
    "\n",
    "```python\n",
    "# Make dummy variables for rank\n",
    "one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)\n",
    "```\n",
    "\n",
    "### Maximum Likelihood\n",
    "- log(ab) = log(a) + log(b)\n",
    "\n",
    "### Cross Entropy\n",
    "A higher cross-entropy implies a lower probability for an event. (cross-entropy is inversely proportional to the total probability of an outcome.)\n",
    "\n",
    "- A good model gives a low cross entropy\n",
    "- A bad model gives a high cross entropy\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{i=1}^m y_i ln(p_i) + (1-y_i) ln (1-p_i)\n",
    "$$\n",
    "\n",
    "#### Coding Cross-entropy\n",
    "```python\n",
    "# Y is for the category, and P is the probability.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "```\n",
    "\n",
    "### Logistic Regression\n",
    "1. Start with random weights: $w_1, ... , w_n, b$\n",
    "2. For every point $(x_1, ... , x_n)$: update $w_i, b$\n",
    "3. Reapeat until error is small\n",
    "\n",
    "### Neural Network Architecture\n",
    "- Input Layer\n",
    "- Hidden Layer\n",
    "- Output Layer\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "### Backpropagation\n",
    "- Doing a feedforward operation.\n",
    "- Comparing the output of the model with the desired output.\n",
    "- Calculating the error.\n",
    "- Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model.\n",
    "- Continue this until we have a model that is good.\n",
    "\n",
    "#### Backpropagate the error\n",
    "$$ (y-\\hat{y}) \\sigma'(x) $$\n",
    "\n",
    "```python\n",
    "def error_term_formula(x, y, output):\n",
    "    return (y - output)*sigmoid_prime(x)\n",
    "```\n",
    "\n",
    "[Lab: Analyzing Student Data](../../notebooks/01%20Introduction%20to%20Neural%20Networks/StudentAdmissions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74087f",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "\n",
    "### Mean Squared Error Function\n",
    "$$\n",
    "E=\\frac{1}{2m}\\sum_{\\mu}(y^{\\mu}-\\hat{y}^{\\mu})^2\n",
    "$$\n",
    "\n",
    "- [Gradient Descent](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent.pdf)\n",
    "- [Gradient Descent Code](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Code.pdf)\n",
    "- [Gradient Descent Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Gradient%20Descent%20Implementing.pdf)\n",
    "- [Multilayer Perceptrons](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Multilayer%20Perceptrons.pdf)\n",
    "- [Backpropagation](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation.pdf)\n",
    "- [Backpropagation Implementing](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/02%20Implementing%20Gradient%20Descent/Backpropagation%20Implementing.pdf)\n",
    "\n",
    "Further reading\n",
    "- From Andrej Karpathy: [Yes, you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)\n",
    "- Also from Andrej Karpathy, [a lecture from Stanford's CS231n course](https://www.youtube.com/watch?v=59Hbtz7XgjM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6672e0",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "- Overfitting -> high variance\n",
    "- Underfitting -> high bias\n",
    "\n",
    "![earlyStopping](./img/earlyStopping.png)\n",
    "\n",
    "### Regularization\n",
    "Large coefficients -> overfitting\n",
    "- L1 Error Function: Good for feature selection\n",
    "$$= -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(|w_1|+...+|w_n|)$$\n",
    "- L2 Error Function: Normally better for training models\n",
    "$$E = -\\frac{1}{m} \\sum_{i=1}^m y_i ln(\\hat{y}_i) + (1-y_i) ln (1-\\hat{y}_i) + \\lambda(w_1^2+...+w_n^2)$$\n",
    "\n",
    "### Dropout\n",
    "Prevent overfitting\n",
    "\n",
    "### Random Restart\n",
    "Jump out the local minima\n",
    "\n",
    "### Vanishing Gradient\n",
    "- Hyperbolic tangent function\n",
    "$$tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "- Rectified Linear Unit (ReLU)\n",
    "$$\n",
    "relu(x)=\n",
    "\\begin{cases}\n",
    "x & if x\\ge 0\\\\\n",
    "0 & if x<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent\n",
    "Decrease training time\n",
    "\n",
    "### Learning Rate Decay\n",
    "Rule:\n",
    "- If steep: long steps\n",
    "- If plain: small steps\n",
    "\n",
    "### Momentum\n",
    "Solve local minmum problem.\n",
    "- STEP: average of previous steps\n",
    "- $\\beta$: momentum\n",
    "- STEP(n) $\\rightarrow$ STEP(n) + $\\beta$ STEP(n-1) + $\\beta^2$ STEP(n-2) + ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37eba8",
   "metadata": {},
   "source": [
    "## [Deep Learning with TensorFlow](https://github.com/udacity/intro-to-ml-tensorflow)\n",
    "\n",
    "### Build Neural Network\n",
    "[Part 1 Introduction to Neural Networks with TensorFlow](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_1_Introduction_to_Neural_Networks_with_TensorFlow_(Solution).ipynb)\n",
    "\n",
    "[Part 2 Neural networks with TensorFlow and Keras](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_2_Neural_networks_with_TensorFlow_and_Keras_(Solution).ipynb)\n",
    "\n",
    "- `tf.multiply()`: Performs element-wise multiplication on two inputs\n",
    "- `tf.matmul()`: Performs matrix multiplication on two inputs\n",
    "- `tf.reduce_sum()`: Computes the sum of elements across an input tensor's dimensions\n",
    "- `tf.convert_to_tensor()`: convert ndarray to a TensorFlow tensor\n",
    "- `tensor.numpy()`: command on the tensor itself to convert it to an ndarray\n",
    "\n",
    "There are [plenty of different datasets](https://www.tensorflow.org/datasets/catalog/overview) available from the `tensorflow_datasets` library, which we shortened in the code to `tfds`. Loading one of the datasets is simple with the `tfds.load()` function, which takes in the dataset name (in this case `mnist`), as well as some other optional arguments such as: 1) the dataset split to get (training, test, validation), 2) whether to shuffle the data, 3) if the data is to be used as part of a supervised learning algorithm (including labels), 4) whether to include metadata about the dataset itself, and [more](https://www.tensorflow.org/datasets/api_docs/python/tfds/load).\n",
    "\n",
    "You can use the `.take()` function with an integer as an argument to get a certain number of images at once from the dataset.\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "- [Pipeline Performance](https://www.tensorflow.org/guide/data_performance)\n",
    "- [Transformations](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "To calculate this probability distribution, we often use the [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Mathematically this looks like\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "TensorFlow also includes one of its own built-in Softmax activation functions you can use. Using the [TensorFlow API documentation]\n",
    "\n",
    "- `tf.nn.softmax`\n",
    "- `tf.math.softmax`\n",
    "- `tf.keras.activations.softmax`\n",
    "\n",
    "#### Neural Networks with TensorFlow\n",
    "\n",
    "Keras helps further simplify working with neural networks running on TensorFlow under the hood. You can more easily stack layers with `tf.keras.Sequential`, making sure to feed an `input_shape` to the first layer of the network. You can also either add separate `Activation` layers, or feed an activation as an argument within certain layers, such as the `Dense` fully-connected layers.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(256, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "#### Subclassing\n",
    "```python\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "        # Define layers \n",
    "        self.input_layer = tf.keras.layers.Flatten()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(256, activation = 'relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(self.num_classes, activation = 'softmax')\n",
    "    \n",
    "    # Define forward Pass   \n",
    "    def call(self, input_tensor):\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a model object\n",
    "subclassed_model = Network(10)\n",
    "\n",
    "# Build the model, i.e. initialize the model's weights and biases\n",
    "subclassed_model.build((None, 28, 28, 1))\n",
    "\n",
    "subclassed_model.summary()\n",
    "```\n",
    "\n",
    "#### Adding Layers with .add\n",
    "\n",
    "Example:\n",
    "```python\n",
    "layer_neurons = [512, 256, 128, 56, 28, 14]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "            \n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "          \n",
    "model.summary() \n",
    "```\n",
    "\n",
    "#### Clearing the Graph\n",
    "\n",
    "In order to avoid clutter from old models in the graph, we can use:\n",
    "\n",
    "```python\n",
    "tf.keras.backend.clear_session()\n",
    "```\n",
    "\n",
    "This command deletes the current `tf.keras` graph and creates a new one.\n",
    "\n",
    "\n",
    "### Train Neural Network\n",
    "[Part 3 Training Neural Networks](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_3_Training_Neural_Networks_(Solution).ipynb)\n",
    "\n",
    "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
    "\n",
    "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "These are the main parameters we are going to set throught these lesson. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "#### Training the Model\n",
    "\n",
    "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:\n",
    "\n",
    "```python\n",
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)\n",
    "```\n",
    "\n",
    "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson. \n",
    "\n",
    "With our model trained, we can check out it's predictions.\n",
    "\n",
    "```python\n",
    "## Build model\n",
    "my_model = tf.keras.Sequential([\n",
    "           tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "           tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "\n",
    "my_model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "## Train model\n",
    "EPOCHS = 5\n",
    "\n",
    "history = my_model.fit(training_batches, epochs = EPOCHS)\n",
    "\n",
    "\n",
    "## Predict model\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = my_model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "```\n",
    "\n",
    "\n",
    "### Train Neural Network on Complex Dataset\n",
    "[Part 4 Fashion MNIST](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_4_Fashion_MNIST_(Solution).ipynb)\n",
    "\n",
    "[Part 5 Inference and Validation](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_5_Inference_and_Validation_(Solution).ipynb)\n",
    "\n",
    "### Inference & Validation\n",
    "\n",
    "We used `tfds.Split.ALL.subsplit` to make a 60/20/20 split for training, validation and test sets, although some TensorFlow datasets have these subsections already built in. Depending on the dataset, you may also want to make sure to shuffle the data at this point as well.\n",
    "\n",
    "Avoid overfitting to the training data?\n",
    "- Stop training when the training and validation curves start to diverge by a certain amount\n",
    "- Save down the best validation accuracy model from during training\n",
    "- Add layers like Dropout to help generalize the network\n",
    "\n",
    "\n",
    "### Saving & Loading\n",
    "[Part 6 Saving and Loading Models](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_6_Saving_and_Loading_Models.ipynb)\n",
    "\n",
    "In TensorFlow we can save our trained models in different formats. Here we will see how to save our models in TensorFlow's SavedModel format and as HDF5 files, which is the format used by Keras models.\n",
    "\n",
    "#### Saving and Loading Models in HDF5 Format\n",
    "\n",
    "To save our models in the format used by Keras models we use the `.save(filepath)` method. For example, to save a model called `my_model` in the current working directory with the name `test_model` we use:\n",
    "\n",
    "```python\n",
    "my_model.save('./test_model.h5')\n",
    "```\n",
    "\n",
    "It's important to note that we have to provide the `.h5` extension to the `filepath` in order the tell `tf.keras` to save our model as an HDF5 file. \n",
    "\n",
    "The above command saves our model into a single HDF5 file that will contain:\n",
    "\n",
    "* The model's architecture.\n",
    "* The model's weight values which were learned during training.\n",
    "* The model's training configuration, which corresponds to the parameters you passed to the `compile` method.\n",
    "* The optimizer and its state. This allows you to resume training exactly where you left off.\n",
    "\n",
    "\n",
    "In the cell below we save our trained `model` as an HDF5 file. The name of our HDF5 will correspond to the current time stamp. This is useful if you are saving many models and want each of them to have a unique name. By default the `.save()` method will **silently** overwrite any existing file at the target location with the same name. If we want `tf.keras` to provide us with a manual prompt to whether overwrite files with the same name, you can set the argument `overwrite=False` in the `.save()` method.\n",
    "\n",
    "```python\n",
    "t = time.time()\n",
    "\n",
    "saved_keras_model_filepath = './{}.h5'.format(int(t))\n",
    "\n",
    "model.save(saved_keras_model_filepath)\n",
    "```\n",
    "\n",
    "Once a model has been saved, we can use `tf.keras.models.load_model(filepath)` to re-load our model. This command will also compile our model automatically using the saved training configuration, unless the model was never compiled in the first place.\n",
    "\n",
    "```python\n",
    "reloaded_keras_model = tf.keras.models.load_model(saved_keras_model_filepath)\n",
    "```\n",
    "\n",
    "#### Saving and Loading TensorFlow SavedModels\n",
    "\n",
    "To export our models to the TensorFlow **SavedModel** format, we use the `tf.saved_model.save(model, export_dir)` function. For example, to save a model called `my_model` in a folder called `saved_models` located in the current working directory we use:\n",
    "\n",
    "```python\n",
    "tf.saved_model.save(my_model, './saved_models')\n",
    "```\n",
    "\n",
    "It's important to note that here we have to provide the path to the directory where we want to save our model, **NOT** the name of the file. This is because SavedModels are not saved in a single file. Rather, when you save your model as a SavedModel, `the tf.saved_model.save()` function will create an `assets` folder, a `variables` folder, and a `saved_model.pb` file inside the directory you provided.\n",
    "\n",
    "The SavedModel files that are created contain:\n",
    "\n",
    "* A TensorFlow checkpoint containing the model weights.\n",
    "* A SavedModel proto containing the underlying TensorFlow graph. Separate graphs are saved for prediction (serving), training, and evaluation. If the model wasn't compiled before, then only the inference graph gets exported.\n",
    "* The model's architecture configuration if available.\n",
    "\n",
    "The SavedModel is a standalone serialization format for TensorFlow objects, supported by TensorFlow serving as well as TensorFlow implementations other than Python. It does not require the original model building code to run, which makes it useful for sharing or deploying in different platforms, such as mobile and embedded devices (with TensorFlow Lite), servers (with TensorFlow Serving), and even web browsers (with TensorFlow.js).\n",
    "\n",
    "In the cell below we save our trained model as a SavedModel. The name of the folder where we are going to save our model will correspond to the current time stamp. Again, this is useful if you are saving many models and want each of them to be saved in a unique directory.\n",
    "\n",
    "```python\n",
    "t = time.time()\n",
    "\n",
    "savedModel_directory = './{}'.format(int(t))\n",
    "\n",
    "tf.saved_model.save(model, savedModel_directory)\n",
    "```\n",
    "\n",
    "Once a model has been saved as a SavedModel, we can use `tf.saved_model.load(export_dir)` to re-load our model. \n",
    "\n",
    "```python\n",
    "reloaded_SavedModel = tf.saved_model.load(savedModel_directory)\n",
    "```\n",
    "\n",
    "It's important to note that the object returned by `tf.saved_model.load` is **NOT** a Keras object. Therefore, it doesn't have `.fit`, `.predict`, `.summary`, etc. methods. It is 100% independent of the code that created it. This means that in order to make predictions with our `reloaded_SavedModel` we need to use a different method than the one used with the re-loaded Keras model.\n",
    "\n",
    "To make predictions on a batch of images with a re-loaded SavedModel we have to use:\n",
    "\n",
    "```python\n",
    "reloaded_SavedModel(image_batch, training=False)\n",
    "```\n",
    "\n",
    "This will return a tensor with the predicted label probabilities for each image in the batch. Again, since we haven't done anything new to this re-loaded SavedModel, then both the `reloaded_SavedModel` and our original `model` should be identical copies. Therefore, they should make the same predictions on the same images.\n",
    "\n",
    "We can also get back a full Keras model, from a TensorFlow SavedModel, by loading our SavedModel with the `tf.keras.models.load_model` function. \n",
    "\n",
    "```python\n",
    "reloaded_keras_model_from_SavedModel = tf.keras.models.load_model(savedModel_directory)\n",
    "```\n",
    "\n",
    "#### Saving Models During Training\n",
    "\n",
    "We have seen that when we train a model with a validation set, the value of the validation loss changes through the training process. Since the value of the validation loss is an indicator of how well our model will generalize to new data, it will be great if could save our model at each step of the training process and then only keep the version with the lowest validation loss. \n",
    "\n",
    "We can do this in `tf.keras` by using the following callback:\n",
    "\n",
    "```python\n",
    "tf.keras.callbacks.ModelCheckpoint('./best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "```\n",
    "This callback will save the model as a Keras HDF5 file after every epoch. With the `save_best_only=True` argument, this callback will first check the validation loss of the latest model against the one previously saved. The callback will only save the latest model and overwrite the old one, if the latest model has a lower validation loss than the one previously saved. This will guarantee that will end up with the version of the model that achieved the lowest validation loss during training.\n",
    "\n",
    "### Loading Images with TensorFlow\n",
    "[Part 7 Loading Image Data](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_7_Loading_Image_Data_(Solution).ipynb)\n",
    "\n",
    "### Data Augmentation\n",
    "`tf.keras` offers many other transformations that we can apply to our images. You can take a look at all the available transformations in the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#arguments)\n",
    "\n",
    "* rotation_range\n",
    "* width_shift_range\n",
    "* height_shift_range\n",
    "* shear_range\n",
    "* zoom_range\n",
    "* horizontal_flip\n",
    "* fill_mode\n",
    "\n",
    "### Creating a Validation Data Generator\n",
    "Generally, we only apply data augmentation to our training data. Therefore, for the validation set we only need to normalize the pixel values of our images.\n",
    "\n",
    "### Pre-Notebooks with GPU\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "[Transfer Learning](https://github.com/stephengineer/Introduction-to-Machine-Learning-with-TensorFlow/blob/main/Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Transfer%20Learning.pdf)\n",
    "\n",
    "[Part 8 Transfer Learning](../../Deep%20Learning/04%20Deep%20Learning%20with%20TensorFlow/Notebooks/Part_8_Transfer_Learning_(Solution).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542be54",
   "metadata": {},
   "source": [
    "## Deep Learning with PyTorch\n",
    "\n",
    "Calculate the output of single layer network using `torch.sum()` or `.sum()` and __matrix multiplication__.\n",
    "\n",
    "### Watch those shapes\n",
    "In general, you'll want to check that the tensors going through your model and other code are the correct shapes. Make use of the `.shape` method during debugging and development.\n",
    "\n",
    "A few things to check if your network isn't training appropriately\n",
    "Make sure you're clearing the gradients in the training loop with `optimizer.zero_grad()`. If you're doing a validation loop, be sure to set the network to evaluation mode with `model.eval()`, then back to training mode with `model.train()`.\n",
    "\n",
    "\n",
    "### CUDA errors\n",
    "Sometimes you'll see this error:\n",
    "\n",
    "```\n",
    "RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #1 ‘mat1’\n",
    "```\n",
    "\n",
    "You'll notice the second type is `torch.cuda.FloatTensor`, this means it's a tensor that has been moved to the GPU. It's expecting a tensor with type `torch.FloatTensor`, no `.cuda` there, which means the tensor should be on the CPU. PyTorch can only perform operations on tensors that are on the same device, so either both CPU or both GPU. If you're trying to run your network on the GPU, check to make sure you've moved the model and all necessary tensors to the GPU with `.to(device)` where `device` is either `\"cuda\"` or `\"cpu\"`.\n",
    "\n",
    "\n",
    "[Tutorial: Deep Learning in PyTorch](http://iamtrask.github.io/2017/01/15/pytorch-tutorial/)\n",
    "\n",
    "[Notebooks](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/05%20Deep%20Learning%20with%20PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ae38a",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "### Normalizing image inputs\n",
    "Data normalization is an important pre-processing step. It ensures that each input (each pixel value, in this case) comes from a standard distribution. That is, the range of pixel values in one input image are the same as the range in another image. This standardization makes our model train and reach a minimum error, faster!\n",
    "\n",
    "\n",
    "### ReLU Activation Function\n",
    "The purpose of an activation function is to scale the outputs of a layer so that they are a consistent, small value. Much like normalizing input values, this step ensures that our model trains efficiently!\n",
    "\n",
    "A ReLU activation function stands for \"Rectified Linear Unit\" and is one of the most commonly used activation functions for hidden layers. It is an activation function, simply defined as the __positive__ part of the input, `x`. So, for an input image with any negative pixel values, this would turn all those values to `0`, black. You may hear this referred to as \"clipping\" the values to zero; meaning that is the lower bound.\n",
    "\n",
    "![ReLU](./img/relu-ex.png)\n",
    "\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "In the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#crossentropyloss), you can see that the cross entropy loss function actually involves two steps:\n",
    "\n",
    "- It first applies a softmax function to any output is sees\n",
    "- Then applies [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss); negative log likelihood loss\n",
    "\n",
    "Then it returns the average loss over a batch of data. Since it applies a softmax function, we do not have to specify that in the `forward` function of our model definition, but we could do this another way.\n",
    "\n",
    "#### Another approach\n",
    "We could separate the softmax and NLLLoss steps.\n",
    "\n",
    "- In the `forward` function of our model, we would explicitly apply a softmax activation function to the output, `x`.\n",
    "\n",
    "```py\n",
    "# a softmax layer to convert 10 outputs into a distribution of class probabilities\n",
    "x = F.log_softmax(x, dim=1)\n",
    "\n",
    "return x\n",
    "```\n",
    "\n",
    "- Then, when defining our loss criterion, we would apply NLLLoss\n",
    "\n",
    "```py\n",
    "# cross entropy loss combines softmax and nn.NLLLoss() in one single class\n",
    "# here, we've separated them\n",
    "criterion = nn.NLLLoss()\n",
    "```\n",
    "\n",
    "This separates the usual `criterion = nn.CrossEntropy()` into two steps: softmax and NLLLoss, and is a useful approach should you want the output of a model to be class probabilities rather than class scores.\n",
    "\n",
    "\n",
    "### Validation Set: Takeaways\n",
    "\n",
    "Measure how well a model generalizes, during training\n",
    "Tell us when to stop training a model; when the validation loss stops decreasing (and especially when the validation loss starts increasing and the training loss is still decreasing)\n",
    "\n",
    "![imageClassificationSteps](./img/imageClassificationSteps.png)\n",
    "\n",
    "### Filters\n",
    "To detect changes in intensity in an image, you’ll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures.\n",
    "\n",
    "### Frequency in images\n",
    "We have an intuition of what frequency means when it comes to sound. High-frequency is a high pitched noise, like a bird chirp or violin. And low frequency sounds are low pitch, like a deep voice or a bass drum. For sound, frequency actually refers to how fast a sound wave is oscillating; oscillations are usually measured in cycles/s ([Hz](https://en.wikipedia.org/wiki/Hertz)), and high pitches and made by high-frequency waves. Examples of low and high-frequency sound waves are pictured below. On the y-axis is amplitude, which is a measure of sound pressure that corresponds to the perceived loudness of a sound, and on the x-axis is time.\n",
    "\n",
    "![frequency](./img/frequency.png)\n",
    "\n",
    "#### High and low frequency\n",
    "Similarly, frequency in images is a __rate of change__. But, what does it means for an image to change? Well, images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. This is easiest to see in an example.\n",
    "\n",
    "![frequencyImage](./img/frequencyImage.png)\n",
    "\n",
    "Most images have both high-frequency and low-frequency components. In the image above, on the scarf and striped shirt, we have a high-frequency image pattern; this part changes very rapidly from one brightness to another. Higher up in this same image, we see parts of the sky and background that change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "__High-frequency components also correspond to the edges of objects in images__, which can help us classify those objects.\n",
    "\n",
    "#### Edge Handling\n",
    "Kernel convolution relies on centering a pixel and looking at it's surrounding neighbors. So, what do you do if there are no surrounding pixels like on an image corner or edge? Well, there are a number of ways to process the edges, which are listed below. It’s most common to use padding, cropping, or extension. In extension, the border pixels of an image are copied and extended far enough to result in a filtered image of the same size as the original image.\n",
    "\n",
    "__Extend__ The nearest border pixels are conceptually extended as far as necessary to provide values for the convolution. Corner pixels are extended in 90° wedges. Other edge pixels are extended in lines.\n",
    "\n",
    "__Padding__ The image is padded with a border of 0's, black pixels.\n",
    "\n",
    "__Crop__ Any pixel in the output image which would require values from beyond the edge is skipped. This method can result in the output image being slightly smaller, with the edges having been cropped.\n",
    "\n",
    "### Pooling layers\n",
    "Some architectures choose to use [average pooling](https://pytorch.org/docs/stable/nn.html#avgpool2d), which chooses to average pixel values in a given window size. So in a 2x2 window, this operation will see 4 pixel values, and return a single, average of those four values, as output!\n",
    "\n",
    "This kind of pooling is typically not used for image classification problems because maxpooling is better at noticing the most important details about edges and other features in an image, but you may see this used in applications for which smoothing an image is preferable.\n",
    "\n",
    "### Padding\n",
    "Padding is just adding a border of pixels around an image. In PyTorch, you specify the size of this border.\n",
    "\n",
    "Why do we need padding?\n",
    "\n",
    "When we create a convolutional layer, we move a square filter around an image, using a center-pixel as an anchor. So, this kernel cannot perfectly overlay the edges/corners of images. The nice feature of padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n",
    "\n",
    "The most common methods of padding are padding an image with all 0-pixels (zero padding) or padding them with the nearest pixel value. You can read more about calculating the amount of padding, given a kernel_size, [here](https://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "### Formula: Number of Parameters in a Convolutional Layer\n",
    "The number of parameters in a convolutional layer depends on the supplied values of `filters/out_channels`, `kernel_size`, and `input_shape`. Let's define a few variables:\n",
    "\n",
    "- `K` - the number of filters in the convolutional layer\n",
    "- `F` - the height and width of the convolutional filters\n",
    "- `D_in` - the depth of the previous layer\n",
    "Notice that `K` = `out_channels`, and `F` = `kernel_size`. Likewise, `D_in` is the last value in the `input_shape` tuple, typically 1 or 3 (RGB and grayscale, respectively).\n",
    "\n",
    "Since there are `F*F*D_in` weights per filter, and the convolutional layer is composed of `K` filters, the total number of weights in the convolutional layer is `K*F*F*D_in`. Since there is one bias term per filter, the convolutional layer has `K` biases. Thus, the __number of parameters__ in the convolutional layer is given by `K*F*F*D_in + K`.\n",
    "\n",
    "### Formula: Shape of a Convolutional Layer\n",
    "The shape of a convolutional layer depends on the supplied values of `kernel_size`, `input_shape`, `padding`, and `stride`. Let's define a few variables:\n",
    "\n",
    "- `K` - the number of filters in the convolutional layer\n",
    "- `F` - the height and width of the convolutional filters\n",
    "- `S` - the stride of the convolution\n",
    "- `P` - the padding\n",
    "- `W_in` - the width/height (square) of the previous layer\n",
    "Notice that `K = out_channels`, `F = kernel_size`, and `S = stride`. Likewise, `W_in` is the first and second value of the `input_shape` tuple.\n",
    "\n",
    "The __depth__ of the convolutional layer will always equal the number of filters `K`.\n",
    "\n",
    "The spatial dimensions of a convolutional layer can be calculated as: `(W_in−F+2P)/S+1`\n",
    "\n",
    "### Optional Resources\n",
    "- Check out the [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) paper!\n",
    "- Read more about [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) here.\n",
    "- The [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) paper can be found here.\n",
    "- Here's the [Keras documentation](https://keras.io/api/applications/) for accessing some famous CNN architectures.\n",
    "- Read this [detailed treatment](http://neuralnetworksanddeeplearning.com/chap5.html) of the vanishing gradients problem.\n",
    "- Here's a [GitHub repository](https://github.com/jcjohnson/cnn-benchmarks) containing benchmarks for different CNN architectures.\n",
    "- Visit the [ImageNet Large Scale Visual Recognition Competition (ILSVRC)](https://image-net.org/challenges/LSVRC/) website.\n",
    "\n",
    "### External Resource\n",
    "[Deep learning eBook](https://www.deeplearningbook.org/) (2016) authored by Ian Goodfellow, Yoshua Bengio, and Aaron Courville; published by Cambridge: MIT Press\n",
    "\n",
    "\n",
    "### 3. Transfer Learning\n",
    "Transfer learning involves taking a pre-trained neural network and adapting the neural network to a new, different data set.\n",
    "\n",
    "Depending on both:\n",
    "\n",
    "- The size of the new data set, and\n",
    "- The similarity of the new data set to the original data set\n",
    "\n",
    "The approach for using transfer learning will be different. There are four main cases:\n",
    "\n",
    "1. New data set is small, new data is similar to original training data.\n",
    "2. New data set is small, new data is different from original training data.\n",
    "3. New data set is large, new data is similar to original training data.\n",
    "4. New data set is large, new data is different from original training data.\n",
    "\n",
    "A large data set might have one million images. A small data could have two-thousand images. The dividing line between a large data set and small data set is somewhat subjective. Overfitting is a concern when using transfer learning with a small data set.\n",
    "\n",
    "Images of dogs and images of wolves would be considered similar; the images would share common characteristics. A data set of flower images would be different from a data set of dog images.\n",
    "\n",
    "Each of the four transfer learning cases has its own approach. In the following sections, we will look at each case one by one.\n",
    "\n",
    "\n",
    "### 4. Weight Initialization\n",
    "It's key to include an element of uniqueness, or __randomness__! Better weights might be selected randomly from within a specified range. By adding variety and all unique weight values, we can ensure that backpropagation will have different activations to look at in the hidden layers, and it can respond to those differences.\n",
    "\n",
    "### Additional Material\n",
    "- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852v1.pdf)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v2.pdf)\n",
    "\n",
    "\n",
    "### 5. Autoencoders\n",
    "\n",
    "Autoencoders are neural networks used for data compression, image de-noising, and dimensionality reduction.\n",
    "\n",
    "\n",
    "### 6. Style Transfer\n",
    "[Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "\n",
    "Content Loss\n",
    "$$ L_{content} = \\frac{1}{2} \\sum (T_c - C_c)^2 $$\n",
    "\n",
    "Style Loss\n",
    "$$ L_{style} = a \\sum_i w_i (T_{s,i} - s_{s,i})^2 $$\n",
    "\n",
    "Total Loss\n",
    "$$ \\alpha L_{content} + \\beta L_{style} $$\n",
    "\n",
    "The smaller alpha-beta ratio ($\\frac{\\alpha}{\\beta}$), the more stylistic effect you will see.\n",
    "\n",
    "\n",
    "### Project: Landmark Classification & Tagging for Social Media\n",
    "\n",
    "Build a landmark classification and tagging system useful for social media!\n",
    "\n",
    "### 8. Deep Learning in Cancer Detection\n",
    "\n",
    "#### Sensitivity and Specificity\n",
    "\n",
    "[Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "Although similar, sensitivity and specificity are not the same as precision and recall. Here are the definitions:\n",
    "\n",
    "In the cancer example, sensitivity and specificity are the following:\n",
    "\n",
    "- Sensitivity: Of all the people **with** cancer, how many were correctly diagnosed?\n",
    "- Specificity: Of all the people **without** cancer, how many were correctly diagnosed?\n",
    "\n",
    "And precision and recall are the following:\n",
    "\n",
    "- Recall: Of all the people who **have cancer**, how many did **we diagnose** as having cancer?\n",
    "- Precision: Of all the people **we diagnosed** with cancer, how many actually **had cancer**?\n",
    "\n",
    "From here we can see that Sensitivity is Recall, and the other two are not the same thing.\n",
    "\n",
    "Trust me, we also have a hard time remembering which one is which, so here's a little trick. If you remember from Luis's Evaluation Metrics section, here is the confusion matrix:\n",
    "\n",
    "![confusion-matrix](./img/confusion-matrix.png)\n",
    "\n",
    "Now, sensitivity and specificity are the rows of this matrix. More specifically, if we label\n",
    "\n",
    "- TP: (True Positives) Sick people that we **correctly** diagnosed as sick.\n",
    "- TN: (True Negatives) Healthy people that we **correctly** diagnosed as healthy.\n",
    "- FP: (False Positives) Healthy people that we **incorrectly** diagnosed as sick.\n",
    "- FN: (False Negatives) Sick people that we **incorrectly** diagnosed as healthy.\n",
    "then:\n",
    "\n",
    "Sensitivity = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "and\n",
    "\n",
    "Specificity = $\\frac{TN}{TN + FP}$\n",
    "\n",
    "![sensitivity-specificity](./img/sensitivity-specificity.png)\n",
    "\n",
    "<center>Sensitivity and Specificity</center>\n",
    "\n",
    "And precision and recall are the top row and the left column of the matrix:\n",
    "\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "and\n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "![precision-recall](./img/precision-recall.png)\n",
    "\n",
    "<center>Precision and Recall</center>\n",
    "\n",
    "The graph below is a histogram of the predictions our model gives in a set of images of lesions, as follows:\n",
    "\n",
    "- Each point in the horizontal axis is a value pp from 0 to 1.\n",
    "- Over each value pp, we locate all the lesions that our classifier predicted to have probability p of being malignant.\n",
    "\n",
    "![threshold](./img/threshold.png)\n",
    "\n",
    "Here we have graphed the thresholds at 0.2, 0.5, and 0.8. Notice how:\n",
    "\n",
    "- At 0.2, we classify every malignant lesion correctly, yet we also send a lot of benign lesions for more testing.\n",
    "- At 0.5, we miss some malignant lesions (bad), and we send a few benign lesions for more testing.\n",
    "- At 0.8, we correctly classify most of the benign lesions, but we miss many malignant lesions (very bad).\n",
    "\n",
    "So in this case, it's arguable that 0.2 is better.\n",
    "\n",
    "\n",
    "#### [ROC Curves](https://www.youtube.com/watch?v=2Iw5TiGzJI4)\n",
    "\n",
    "The curves have been introduced as follows, where in the horizontal axis we plot the True Positive Rate, and in the vertical axis we plot the False Positive Rate.\n",
    "\n",
    "![roc-1](./img/roc-1.png)\n",
    "\n",
    "![roc](./img/roc.png)\n",
    "\n",
    "However, you'll see that in this section, I will use a different ROC Curve. The one I use looks like I flipped it sideways, like this:\n",
    "\n",
    "![roc-curve](./img/roc-curve.png)\n",
    "\n",
    "\n",
    "And there's a really cool reason why I use this one. And it's because it's the curve we get when we plot the sensitivity in the horizontal axis, and the specificity in the vertical axis!\n",
    "\n",
    "Let me be more specific (yes pun intended). Let's use the same histogram as in the last section.\n",
    "\n",
    "![threshold-1](./img/threshold-1.png)\n",
    "\n",
    "Recall that the values in the horizontal axis are all the possible thresholds. For any threshold pp between 0 and 1, the verdict of the model will be the following: \"*Any lesion to the left of this threshold will be considered benign, and any lesion to the right of this threshold will be considered malignant, and sent for more tests*.\"\n",
    "\n",
    "Now, for this particular model, we calculate the sensitivity and specificity as follows:\n",
    "\n",
    "- Sensitivity: Out of all the malignant lesions, what percentage are to the right of the threshold (correctly classified)?\n",
    "- Specificity: Out of all the benign lesions, what percentage are to the left of the threshold (correctly classified)?\n",
    "\n",
    "And we plot that point, where the coordinates are (Sensitivity, Specificity). If we plot all the points corresponding to each of the possible thresholds between 0% and 100%, we'll get the ROC curve that I drew above. Therefore, we can also refer to the ROC curve as the *Sensitivity-Specificity Curve*.\n",
    "\n",
    "And finally, here's a little animation of the ROC curve getting drawn, as the threshold moves from 0 to 1.\n",
    "\n",
    "\n",
    "#### Confusion Matrices\n",
    "\n",
    "In Luis's Evaluation Metrics section, we learned about confusion matrices, and if you need a refresher, the [video](https://www.youtube.com/watch?v=9GLNjmMUB_4).\n",
    "\n",
    "##### Type 1 and Type 2 Errors\n",
    "\n",
    "Sometimes in the literature, you'll see False Positives and True Negatives as Type 1 and Type 2 errors. Here is the correspondence:\n",
    "\n",
    "- **Type 1 Error (Error of the first kind, or False Positive):** In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "- **Type 2 Error (Error of the second kind, or False Negative):** In the medical example, this is when we misdiagnose a sick patient as healthy.\n",
    "\n",
    "But confusion matrices can be much larger than 2 X 2. Here's an example of a larger one. Let's say we have three illnesses called A, B, C. And here is a confusion matrix:\n",
    "\n",
    "![new-confusion-matrix](./img/new-confusion-matrix.png)\n",
    "\n",
    "<center>A confusion matrix for three types of illnesses: A, B, and C</center>\n",
    "\n",
    "As you can see, each entry in the ii-th row and the j-th column will tell you the probability of the patient having illness ii and getting diagnosed with illness j.\n",
    "\n",
    "For example, from the entry on the second row and the first column, we can determine that if a patient has illness B, the probability of getting diagnosed with illness A is exactly 0.08.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b57458",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "\n",
    "### 1. Recurrent Neural Networks\n",
    "\n",
    "[Sketch RNN (demo here)](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html) is a program that learns to complete a drawing, once you give it something (a line or circle, etc.) to start!\n",
    "\n",
    "\n",
    "#### A bit of history\n",
    "RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the \"—__vanishing gradient__\" problem in which the contribution of information decays geometrically over time.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "As you may recall, while training our network we use __backpropagation__. In the backpropagation process we adjust our weight matrices with the use of a __gradient__. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\".\n",
    "\n",
    "__LSTM__ is one option to overcome the Vanishing Gradient problem in RNNs.\n",
    "\n",
    "Please use these resources if you would like to read more about the [Vanishing Gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem or understand further the concept of a [Geometric Series](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions) and how its values may exponentially decrease.\n",
    "\n",
    "If you are still curious, for more information on the important milestones mentioned here, please take a peek at the following links:\n",
    "\n",
    "- [TDNN](https://en.wikipedia.org/wiki/Time_delay_neural_network)\n",
    "- Here is the original [Elman Network](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).\n",
    "- In this [LSTM](http://www.bioinf.jku.at/publications/older/2604.pdf) link you will find the original paper written by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [Jürgen Schmidhuber](https://people.idsia.ch//~juergen/). Don't get into all the details just yet. We will cover all of this later!\n",
    "\n",
    "As mentioned in the video, Long Short-Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies. In this lesson we will focus on RNNs and continue with LSTMs. We will not be focusing on GRUs. More information about GRUs can be found in the following blog. Focus on the overview titled: __GRUs__.\n",
    "\n",
    "\n",
    "#### Applications\n",
    "There are so many interesting applications, let's look at a few more!\n",
    "\n",
    "- Are you into gaming and bots? Check out the [DotA 2 bot by Open AI](https://openai.com/blog/dota-2/)\n",
    "- How about [automatically adding sounds to silent movies?](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)\n",
    "- Here is a cool tool for [automatic handwriting generation]()\n",
    "- Amazon's voice to text using high quality speech recognition, [Amazon Lex](https://aws.amazon.com/lex/faqs/).\n",
    "- Facebook uses RNN and LSTM technologies for [building language models](https://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/)\n",
    "- Netflix also uses RNN models: [here is an interesting read](https://arxiv.org/pdf/1511.06939.pdf)\n",
    "\n",
    "\n",
    "#### Feedforward Neural Network - A Reminder\n",
    "The mathematical calculations needed for training RNN systems are fascinating. To deeply understand the process, we first need to feel confident with the vanilla FFNN system. We need to thoroughly understand the feedforward process, as well as the backpropagation process used in the training phases of such systems. The next few videos will cover these topics, which you are already familiar with. We will address the feedforward process as well as backpropagation, using specific examples. These examples will serve as extra content to help further understand RNNs later in this lesson.\n",
    "\n",
    "The following couple of videos will give you a brief overview of the __Feedforward Neural Network (FFNN)__.\n",
    "\n",
    "As mentioned before, when working with neural networks we have 2 primary phases: __Training__ and __Evaluation__.\n",
    "\n",
    "During the __training__ phase, we take the data set (also called the training set), which includes many pairs of inputs and their corresponding targets (outputs). Our goal is to find a set of weights that would best map the inputs to the desired outputs. In the __evaluation__ phase, we use the network that was created in the training phase, apply our new inputs and expect to obtain the desired outputs.\n",
    "\n",
    "The training phase will include two steps: __Feedforward__ and __Backpropagation__\n",
    "\n",
    "We will repeat these steps as many times as we need until we decide that our system has reached the best set of weights, giving us the best possible outputs.\n",
    "\n",
    "The next two videos will focus on the feedforward process.\n",
    "\n",
    "You will notice that in these videos I use subscripts as well as superscript as a numeric notation for the weight matrix.\n",
    "\n",
    "For example:\n",
    "\n",
    "- $W_k$ is weight matrix k\n",
    "- $W_{ij}^{k}$ is the ij element of weight matrix k\n",
    "\n",
    "\n",
    "#### Feedforward\n",
    "In this section we will look closely at the math behind the feedforward process. With the use of basic Linear Algebra tools, these calculations are pretty simple!\n",
    "\n",
    "If you are not feeling confident with linear combinations and matrix multiplications, you can use the following links as a refresher:\n",
    "\n",
    "- [Linear Combination](http://linear.ups.edu/html/section-LC.html)\n",
    "- [Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "\n",
    "Assuming that we have a single hidden layer, we will need two steps in our calculations. The first will be calculating the value of the hidden states and the latter will be calculating the value of the outputs.\n",
    "\n",
    "Notice that both the hidden layer and the output layer are displayed as vectors, as they are both represented by more than a single neuron.\n",
    "\n",
    "##### Calculating the value of the hidden states\n",
    "\n",
    "[Video](https://youtu.be/4rCfnWbx8-0)\n",
    "\n",
    "vector h' of the hidden layer will be calculated by multiplying the input vector with the weight matrix $W^{1}$ the following way: $\\bar{h'}=(\\bar{x}W^1)$ Using vector by matrix multiplication.\n",
    "\n",
    "After finding h' we need an activation function $\\Phi$ to finalize the computation of the hidden layer's values. This activation function can be a Hyperbolic Tangent, a Sigmoid or a ReLU function. We can use the following two equations to express the final hidden vector $\\bar{h}$: $\\bar{h}=\\Phi(\\bar{x}W^1)$ or $\\bar{h}=\\Phi(h')$\n",
    "\n",
    "\n",
    "More information on the activation functions and how to use them can be found [here](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)\n",
    "\n",
    "\n",
    "##### Calculating the values of the Outputs.\n",
    "\n",
    "[Video](https://youtu.be/kTYbTVh1d0k)\n",
    "\n",
    "The process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication, which can be followed by an activation function. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.\n",
    "\n",
    "Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer.\n",
    "\n",
    "In our example, the input vector is $\\bar{h}$ and the matrix is $W^2$, therefore $\\bar{y}=\\bar{h}W^2$. In some applications it can be beneficial to use a softmax function (if we want all output values to be between zero and 1, and their sum to be 1).\n",
    "\n",
    "The two error functions that are most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (usually used in classification problems).\n",
    "\n",
    "In the above calculations we used a variation of the MSE.\n",
    "\n",
    "The next few videos will focus on the backpropagation process, or what we also call stochastic gradient decent with the use of the chain rule.\n",
    " \n",
    "\n",
    "#### Backpropagation Theory\n",
    "\n",
    "Since partial derivatives are the key mathematical concept used in backpropagation, it's important that you feel confident in your ability to calculate them. Once you know how to calculate basic derivatives, calculating partial derivatives is easy to understand.\n",
    "For more information on partial derivatives use the following [link](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)\n",
    "\n",
    "For calculation purposes in future quizzes of the lesson, you can use the following link as a reference for [common derivatives](https://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf).\n",
    "\n",
    "In the __backpropagation__ process we minimize the network error slightly with each iteration, by adjusting the weights. The following video will help you understand the mathematical process we use for computing these adjustments.\n",
    "\n",
    "If we look at an arbitrary layer k, we can define the amount by which we change the weights from neuron i to neuron j stemming from layer k as: $\\Delta W^k_{ij}$\n",
    "\n",
    "The superscript (k) indicates that the weight connects layer k to layer k+1.\n",
    "\n",
    "Therefore, the weight update rule for that neuron can be expressed as:\n",
    "\n",
    "$W_{new} = W_{previous} + \\Delta W^k_{ij}$\n",
    "\n",
    "The updated value $\\Delta W_{ij}^k$ is calculated through the use of the gradient calculation, in the following way:\n",
    "\n",
    "$\\Delta W_{ij}^k=\\alpha (-\\frac{\\partial E}{\\partial W})$, where $\\alpha$ is a small positive number called the __Learning Rate__.\n",
    "\n",
    "From these derivation we can easily see that the weight updates are calculated the by the following equation:\n",
    "\n",
    "$W_{new} = W_{previous} + \\alpha (-\\frac{\\partial E}{\\partial W})$\n",
    "\n",
    "Since many weights determine the network’s output, we can use a vector of the partial derivatives (defined by the Greek letter Nabla $\\nabla$) of the network error - each with respect to a different weight.\n",
    "\n",
    "$W_{new} = W_{previous} + \\alpha \\nabla w (-E)$\n",
    "\n",
    "Here you can find other good resources for understanding and tuning the Learning Rate:\n",
    "\n",
    "- [resource 1](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)\n",
    "- [resource 2](https://cs231n.github.io/neural-networks-3/#loss)\n",
    "\n",
    "\n",
    "#### RNN\n",
    "\n",
    "RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of the feedforward and backpropagation steps that are used in the training phase.\n",
    "\n",
    "There are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:\n",
    "\n",
    "- __sequences__ as inputs in the training phase, and\n",
    "- __memory__ elements\n",
    "\n",
    "Memory is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training step.\n",
    "\n",
    "The basic three layer neural network with feedback that serve as memory inputs is called the __Elman Network__ and is depicted in the following picture:\n",
    "\n",
    "![Elman-Network](./img/ElmanNetwork.png)\n",
    "\n",
    "As mentioned in the History concept, here is the original [Elman Network](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).\n",
    "\n",
    "As we've see, in FFNN the output at any time t, is a function of the current input and the weights. This can be easily expressed using the following equation:\n",
    "\n",
    "$\\bar{y}_t = F (\\bar{x}_t, W)$\n",
    "\n",
    "In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:\n",
    "\n",
    "$\\bar{y}_t = F(\\bar{x}_t, \\bar{x}_{t-1}, \\bar{x}_{t-2}, ..., , \\bar{x}_{t-t_0}, W)$\n",
    "\n",
    "This is the RNN __folded model__:\n",
    "\n",
    "![The RNN folded model](./img/foldedModel.png)\n",
    "\n",
    "In this picture, $\\bar{x}$ represents the input vector, $\\bar{y}$ represents the output vector and $\\bar{s}$ denotes the state vector.\n",
    "\n",
    "$W_x$ is the weight matrix connecting the inputs to the state layer.\n",
    "\n",
    "$W_y$ is the weight matrix connecting the state layer to the output layer.\n",
    "\n",
    "$W_s$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "The model can also be \"unfolded in time\". The __unfolded model__ is usually what we use when working with RNNs.\n",
    "\n",
    "![The RNN unfolded model](./img/unfoldedModel.png)\n",
    "\n",
    "In both the folded and unfolded models shown above the following notation is used:\n",
    "\n",
    "$\\bar{x}$ represents the input vector, $\\bar{y}$ represents the output vector and $\\bar{s}$ represents the state vector.\n",
    "\n",
    "$W_x$ is the weight matrix connecting the inputs to the state layer.\n",
    "\n",
    "$W_y$ is the weight matrix connecting the state layer to the output layer.\n",
    "\n",
    "$W_s$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function $\\Phi$ in the following way:\n",
    "\n",
    "$\\bar{h}=\\Phi(\\bar{x}W)$ \n",
    "\n",
    "In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and __also__ on the previous state:\n",
    "\n",
    "$\\bar{s}_t = \\Phi (\\bar{x}_t W_x + \\bar{s}_{t-1} W_s)$\n",
    "\n",
    "The output vector is calculated exactly the same as in FFNNs. It can be a linear combination of the inputs to each output node with the corresponding weight matrix $W_y$, or a softmax function of the same linear combination.\n",
    "\n",
    "$\\bar{y}_t=\\bar{s}_t W_y$\n",
    "\n",
    "or\n",
    "\n",
    "$\\bar{y}_t=\\sigma(\\bar{s}_t W_y)$\n",
    "\n",
    "\n",
    "#### Backpropagation Through Time\n",
    "\n",
    "The Loss Function is the square of the difference between the desired and the calculated outputs. There are variations to the Loss Function, for example, factoring it with a scalar. In the backpropagation example we used a factoring scalar of 1/2 for calculation convenience.\n",
    "\n",
    "As described previously, the two most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (usually used in classification problems).\n",
    "\n",
    "If we backpropagate more than ~10 timesteps, the gradient will become too small. This phenomena is known as the vanishing gradient problem where the contribution of information decays geometrically over time. Therefore temporal dependencies that span many time steps will effectively be discarded by the network. Long Short-Term Memory (LSTM) cells were designed to specifically solve this problem.\n",
    "\n",
    "In RNNs we can also have the opposite problem, called the exploding gradient problem, in which the value of the gradient grows uncontrollably. A simple solution for the exploding gradient problem is Gradient Clipping.\n",
    "\n",
    "\n",
    "### 2. Long Short-Term Memory Networks (LSTMs)\n",
    "\n",
    "- [Chris Olah's LSTM post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Edwin Chen's LSTM post](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "- [Andrej Karpathy's lecture](https://youtu.be/iX5V1WpxxkY) on RNNs and LSTMs from CS231n\n",
    "\n",
    "#### Learn Gate\n",
    "\n",
    "The output of the Learn Gate is $N_ti_t$ where:\n",
    "\n",
    "$N_t = tanh(W_n [STM_{t-1}, E_t] + b_n)$\n",
    "\n",
    "$i_t = \\sigma (W_t [STM_{t-1}, E_t] + b_i)$\n",
    "\n",
    "#### Forget Gate\n",
    "\n",
    "The output of the Forget Gate is $LTM_{t-1}f_t$ where:\n",
    "  \n",
    "$f_t = \\sigma (W_f [STM_{t-1}, E_t] + b_f)$\n",
    "\n",
    "#### Remember Gate\n",
    "\n",
    "The output of the Remember Gate is:\n",
    "\n",
    "$LTM_{t-1} + N_ti_t$\n",
    "\n",
    "#### Use Gate\n",
    "\n",
    "The output of the Use Gate is $U_t V_t$ where:\n",
    "\n",
    "$U_t = tanh(W_uLTM_{t-1}f_t + b_u)$\n",
    "\n",
    "$V_t = \\sigma (W_v[STM_{t-1}, E_t] + b_t)$\n",
    "\n",
    "\n",
    "[A Beginner's Guide to LSTMs and Recurrent Neural Networks](https://web.archive.org/web/20190106151528/https://skymind.ai/wiki/lstm)\n",
    "\n",
    "![lstm](./img/lstm.png)\n",
    "\n",
    "![lstm](./img/lstm-2.png)\n",
    "\n",
    "#### Gated Recurrent Unit (GRU)\n",
    "\n",
    "Additional information about GRUs can be found in the following links:\n",
    "\n",
    "[Michael Guerzhoy's post](http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf)\n",
    "\n",
    "\n",
    "### 3. Implementation of RNN and LSTM\n",
    "\n",
    "Hidden state should have dimensions: num_layers, batch_size, hidden_dim\n",
    "\n",
    "\n",
    "### 4. Hyperparameters\n",
    "\n",
    "- Optimizer Hyperparameters: learning rate, minibatch size, epoch\n",
    "- Model Hyperparamters: numbers of layers, hidden units, model specific hyperparameters for architectures\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "Good starting point = 0.01\n",
    "\n",
    "Adaptive Learning Optimizers:\n",
    "- AdamOptimizer\n",
    "- AdagradOptimizer\n",
    "\n",
    "#### Minibatch Size\n",
    "\n",
    "[Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas\n",
    "\n",
    "Minibatch size (# of training example): 32, 64, 128, 256\n",
    "\n",
    "#### Number of Hidden Unites/Layers\n",
    "\n",
    "\"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers).\" ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "While some tasks show reasonable performance with embedding sizes between 50-200, it's not unusual to see it go up 500 or even 1000.\n",
    "\n",
    "\n",
    "### 5. Embeddings & Word2Vec\n",
    "\n",
    "Supporting Materials\n",
    "- [Word2Vec Mikolov](https://video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf)\n",
    "- [Distributed Representations, Mikolov 2](https://video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf)\n",
    "\n",
    "Subsampling equation\n",
    "\n",
    "$P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$\n",
    "\n",
    "### 6. Sentiment Prediction RNN\n",
    "\n",
    "\n",
    "### Project: Generate TV Scripts\n",
    "\n",
    "\n",
    "### 8. Attention\n",
    "\n",
    "#### Encoders and Decoders\n",
    "The encoder and decoder do not have to be RNNs; they can be CNNs too!\n",
    "\n",
    "In the example above, an LSTM is used to generate a sequence of words; LSTMs \"remember\" by keeping track of the input words that they see and their own hidden state.\n",
    "\n",
    "In computer vision, we can use this kind of encoder-decoder model to generate words or captions for an input image or even to generate an image from a sequence of input words. We'll focus on the first case: generating captions for images, and you'll learn more about caption generation in the next lesson. For now know that we can input an image into a CNN (encoder) and generate a descriptive caption for that image using an LSTM (decoder).\n",
    "\n",
    "- The encoder and decoder are usually RNNs in NLP applications.\n",
    "- Many real-world models use embeddings of sizes close to 200 or 300.\n",
    "- The decoder is the right place for calculating attention!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2688b8",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "\n",
    "### 1. Generative Adversarial Networks\n",
    "Links to Resources\n",
    "- [StackGAN](https://arxiv.org/abs/1612.03242) realistic image synthesis\n",
    "- [iGAN](https://github.com/junyanz/iGAN) interactive image generation\n",
    "- [CartoonGAN](https://video.udacity-data.com/topher/2018/November/5bea23cd_cartoongan/cartoongan.pdf)\n",
    "- [Improved-training-techniques](https://video.udacity-data.com/topher/2018/November/5bea0c6a_improved-training-techniques/improved-training-techniques.pdf)\n",
    "- [interesting applications](https://jonathan-hui.medium.com/gan-some-cool-applications-of-gans-4c9ecca35900)\n",
    "\n",
    "\n",
    "### 2. Deep Convolutional GANs\n",
    "\n",
    "DCGAN Paper\n",
    "It's always good to take a look at the original paper when you can. Many papers discuss both the theory and training details of deep learning networks, and you can read the DCGAN paper, [Unsupervised Representational Learning with Deep Convolutional Generative Adversarial Networks, at this link](https://arxiv.org/pdf/1511.06434.pdf). I especially like the section they have on model architectures, which is pasted for convenience as an image, below.\n",
    "\n",
    "![ArchitectureDCGANs](./img/ArchitectureDCGANs.png)\n",
    "\n",
    "#### What is Batch Normalization?\n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to every layer within the network.\n",
    "\n",
    "#### Batch Normalization\n",
    "It's called \"batch\" normalization because, during __training__, we normalize each layer's inputs by using the mean and standard deviation (or variance) of the values in the current batch. These are sometimes called the batch statistics.\n",
    "\n",
    "Specifically, batch normalization normalizes the output of a previous layer by __subtracting the batch mean and dividing by the batch standard deviation__.\n",
    "\n",
    "Why might this help? Well, we know that normalizing the inputs to a network helps the network learn and converge to a solution. However, a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the first layer of a smaller network.\n",
    "\n",
    "#### Normalization at Every Layer\n",
    "For example, imagine a 3 layer network.\n",
    "\n",
    "![3-layers](./img/3-layers.png)\n",
    "\n",
    "Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network.\n",
    "\n",
    "![2-layers](./img/2-layers.png)\n",
    "\n",
    "Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.\n",
    "\n",
    "![one-layer](./img/one-layer.png)\n",
    "\n",
    "When you think of it like this - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at __every layer (sub-network)__.\n",
    "\n",
    "#### Internal Covariate Shift\n",
    "Beyond the intuitive reasons, there are good mathematical reasons to motivate batch normalization. It helps combat what the authors call __internal covariate shift__.\n",
    "\n",
    "In this case, internal covariate shift refers to the change in the distribution of the inputs to different layers. It turns out that training a network is most efficient when the distribution of inputs to each layer is similar!\n",
    "\n",
    "And batch normalization is one method of standardizing the distribution of layer inputs. This discussion is best handled [in the paper](https://arxiv.org/pdf/1502.03167.pdf) and in [Deep Learning](https://www.deeplearningbook.org/), a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of [Chapter 8: Optimization for Training Deep Models](https://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "#### The Math\n",
    "Next, let's do a deep dive into the math behind batch normalization. This is not critical for you to know, but it may help your understanding of this whole process!\n",
    "\n",
    "##### Getting the mean and variance\n",
    "\n",
    "In order to normalize the values, we first need to find the average value for the batch. If you look at the code, you can see that this is not the average value of the batch inputs, but the average value coming out of any particular layer before we pass it through its non-linear activation function and then feed it as an input to the next layer.\n",
    "\n",
    "We represent the average as __mu_B__\n",
    "\n",
    "$\\mu_B$\n",
    "\n",
    "which is simply the sum of all of the values, __x_i__ divided by the number of values, __m__.\n",
    "\n",
    "$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^m x_i$\n",
    "\n",
    "We then need to calculate the variance, or mean squared deviation, represented as\n",
    "\n",
    "$\\sigma_{B}^{2}σ$\n",
    "\n",
    "If you aren't familiar with statistics, that simply means for each value __x_i__, we subtract the average value (calculated earlier as __mu_B__), which gives us what's called the \"deviation\" for that value. We square the result to get the squared deviation. Sum up the results of doing that for each of the values, then divide by the number of values, again __m__, to get the average, or mean, squared deviation.\n",
    "\n",
    "$\\sigma_{B}^{2} \\leftarrow \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_B)^2$\n",
    "\n",
    "#### Normalizing output values\n",
    "Once we have the mean and variance, we can use them to normalize the values with the following equation. For each value, it subtracts the mean and divides by the (almost) standard deviation. (You've probably heard of standard deviation many times, but if you have not studied statistics you might not know that the standard deviation is actually the square root of the mean squared deviation.)\n",
    "\n",
    "$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}$\n",
    "\n",
    "Above, we said \"(almost) standard deviation\". That's because the real standard deviation for the batch is calculated by\n",
    "\n",
    "$\\sqrt{\\sigma_{B}^{2}}$\n",
    "\n",
    "but the above formula adds the term epsilon before taking the square root. The epsilon can be any small, positive constant, ex. the value `0.001`. It is there partially to make sure we don't try to divide by zero, but it also acts to increase the variance slightly for each batch.\n",
    "\n",
    "Why add this extra value and mimic an increase in variance? Statistically, this makes sense because even though we are normalizing one batch at a time, we are also trying to estimate the population distribution – the total training set, which itself an estimate of the larger population of inputs your network wants to handle. The variance of a population is typically higher than the variance for any sample taken from that population, especially when you use a small sample size (a small sample is more likely to include values near the peak of a population distribution), so increasing the variance a little bit for each batch helps take that into account.\n",
    "\n",
    "At this point, we have a normalized value, represented as\n",
    "\n",
    "$\\hat{x_i}$\n",
    "\n",
    "But rather than use it directly, we multiply it by a __gamma__ value, and then add a __beta__ value. Both gamma and beta are learnable parameters of the network and serve to scale and shift the normalized value, respectively. Because they are learnable just like weights, they give your network some extra knobs to tweak during training to help it learn the function it is trying to approximate.\n",
    "\n",
    "$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta$\n",
    "\n",
    "We now have the final batch-normalized output of our layer, which we would then pass to a non-linear activation function like sigmoid, tanh, ReLU, Leaky ReLU, etc. In the original batch normalization paper, they mention that there might be cases when you'd want to perform the batch normalization after the non-linearity instead of before, but it is difficult to find any uses like that in practice.\n",
    "\n",
    "\n",
    "#### Adding Batch Normalization Layers to a PyTorch Model\n",
    "In the last notebook, you saw how a model with batch normalization applied reached a lower training loss and higher test accuracy! There are quite a few comments in that code, and I just want to recap a few of the most important lines.\n",
    "\n",
    "To add batch normalization layers to a PyTorch model:\n",
    "\n",
    "- You add batch normalization to layers inside the `__init__` function.\n",
    "- Layers with batch normalization do not include a bias term. So, for linear or convolutional layers, you'll need to set `bias=False` if you plan to add batch normalization on the outputs.\n",
    "- You can use PyTorch's [BatchNorm1d] function to handle the math on linear outputs or [BatchNorm2d] for 2D outputs, like filtered images from convolutional layers.\n",
    "- You add the batch normalization layer before calling the activation function, so it always goes layer > batch norm > activation.\n",
    "\n",
    "Finally, when you tested your model, you set it to `.eval()` mode, which ensures that the batch normalization layers use the populationrather than the batch mean and variance (as they do during training).\n",
    "\n",
    "##### The takeaway\n",
    "By using batch normalization to normalize the inputs at each layer of a network, we can make these inputs more consistent and thus reduce oscillations that may happen in gradient descent calculations. This helps us build deeper models that also converge faster!\n",
    "\n",
    "Take a look at the [PyTorch BatchNorm2d documentation](https://pytorch.org/docs/stable/nn.html#batchnorm2d) to learn more about how to add batch normalization to a model, and how data is transformed during training (and evaluation).\n",
    "\n",
    "#### Benefits of Batch Normalization\n",
    "Batch normalization optimizes network training. It has been shown to have several benefits:\n",
    "\n",
    "1. Networks train faster – Each training iteration will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall.\n",
    "2. Allows higher learning rates – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.\n",
    "3. Makes weights easier to initialize – Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.\n",
    "4. Makes more activation functions viable – Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.\n",
    "5. Simplifies the creation of deeper networks – Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.\n",
    "6. Provides a bit of regularization – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network.\n",
    "7. May give better results overall – Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization.\n",
    "\n",
    "\n",
    "#### Why no bias?\n",
    "The reason there is no `bias` for our convolutional layers is because we have batch normalization applied to their outputs. The goal of batch normalization is to get outputs with:\n",
    "\n",
    "- mean = 0\n",
    "- standard deviation = 1\n",
    "\n",
    "Since we want the mean to be 0, we do not want to add an offset (bias) that will deviate from 0. We want the outputs of our convolutional layer to rely only on the coefficient weights.\n",
    "\n",
    "\n",
    "### 3. Pix2Pix & CycleGAN\n",
    "Links to Related Work\n",
    "- Ian Goodfellow's [original paper on GANs](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)\n",
    "- Face swap with [CycleGAN Face-off](https://arxiv.org/pdf/1712.03451.pdf)\n",
    "\n",
    "#### Objective Loss Functions\n",
    "An objective function is typically a loss function that you seek to minimize (or in some cases maximize) during training a neural network. These are often expressed as a function that measures the difference between a prediction y_hat and a true target y.\n",
    "\n",
    "$\\mathcal{L} (y, \\hat{y})$\n",
    "\n",
    "The objective function we've used the most in this program is cross entropy loss, which is a negative log loss applied to the output of a softmax layer. For a binary classification problem, as in real or fake image data, we can calculate the binary cross entropy loss as:\n",
    "\n",
    "$-[y\\log(\\hat{y}) +(1-y) \\log (1-\\hat{y})]$\n",
    "\n",
    "In other words, a sum of two log losses!\n",
    "\n",
    "In the notation in the next video, you'll see that y_hat is the output of the discriminator; our predicted class.\n",
    "\n",
    "#### Latent Space\n",
    "Latent means \"hidden\" or \"concealed\". In the context of neural networks, a latent space often means a feature space, and a latent vector is just a compressed, feature-level representation of an image!\n",
    "\n",
    "For example, when you created a simple autoencoder, the outputs that connected the encoder and decoder portion of a network made up a compressed representation that could also be referred to as a latent vector.\n",
    "\n",
    "You can read more about latent space in [this blog post] as well as an interesting property of this space: recall that we can mathematically operate on vectors in vector space and with latent vectors, we can perform a kind of feature-level transformation on an image!\n",
    "\n",
    "This manipulation of latent space has even been used to create an [interactive GAN, iGAN](https://github.com/junyanz/iGAN/blob/master/README.md) for interactive image generation! I recommend reading the paper, linked in the Github readme.\n",
    "\n",
    "#### Pix2Pix resources\n",
    "If you're interested in learning more, take a look at the [original Pix2Pix paper](https://arxiv.org/pdf/1611.07004.pdf). I'd also recommend this related work on creating high-res images: [high resolution, conditional GANs](https://tcwang0509.github.io/pix2pixHD/).\n",
    "\n",
    "#### Edges to Cats Demo\n",
    "Try out Christopher Hesse's [image-to-image demo](https://affinelayer.com/pixsrv/) to get some really interesting (and sometimes creepy) results!\n",
    "\n",
    "\n",
    "### 4. Implementing a CycleGAN\n",
    "\n",
    "\n",
    "### Project: Generate Faces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e34d34",
   "metadata": {},
   "source": [
    "## Deploying a Model\n",
    "\n",
    "\n",
    "### 1. Introduction to Deployment\n",
    "\n",
    "You will put these ideas to practice by using [Amazon's SageMaker](https://aws.amazon.com/sagemaker/). SageMaker is just one method for deploying machine learning models.\n",
    "\n",
    "There are three parts, and they are to explore and process the data, model your data, and then deploy your model.\n",
    "\n",
    "![machine-learning-workflow](./img/machine-learning-workflow.png)\n",
    "\n",
    "\n",
    "#### References\n",
    "Below are links that provide more detailed information on the Machine Learning Workflow that we discussed in this section above, described by cloud providers: Amazon, Google, and Microsoft.\n",
    "\n",
    "1. [Amazon Web Services](https://aws.amazon.com/) (AWS) discusses their definition of the [Machine Learning Workflow](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-mlconcepts.html).\n",
    "2. [Google Cloud Platform](https://cloud.google.com/) (GCP) discusses their definition of the [Machine Learning Workflow](https://cloud.google.com/ai-platform/docs/ml-solutions-overview).\n",
    "3. [Microsoft Azure](https://azure.microsoft.com/en-us/) (Azure) discusses their definition of the [Machine Learning Workflow](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines).\n",
    "\n",
    "\n",
    "#### Cloud Computing\n",
    "\n",
    "Cloud computing can simply be thought of as transforming an Information Technology (IT) product into a service. With our vacation photos example, we transformed storing photos on an IT product, the flash drive; into storing them using a service, like Google Drive.\n",
    "\n",
    "Using a cloud storage service provides the benefits of making it easier to access and share your vacation photos, because you no longer need the flash drive. You’ll only need a device with an internet connection to access your photos and to grant permission to others to access your photos.\n",
    "\n",
    "Generally, think of cloud computing as using an internet connected device to log into a cloud computing service, like Google Drive, to access an IT resource, your vacation photos. These IT resources, your vacation photos, are stored in the cloud provider’s data center. Besides cloud storage, other cloud services include: cloud applications, databases, virtual machines, and other services like SageMaker.\n",
    "\n",
    "\n",
    "##### Why would a business decide to use cloud computing?\n",
    "Most of the factors related to choosing cloud computing services, instead of developing on-premise IT resources are related to time and cost. The capacity utilization graph below shows how cloud computing compares to traditional infrastructure (on-premise IT resources) in meeting customer demand.\n",
    "\n",
    "Capacity in the graph below can be thought of as the IT resources like: compute capacity, storage, and networking, that's needed to meet customer demand for a business' products and the costs associated with those IT resources. In our vacation photos example, customer demand is for storing and sharing customer photos. The IT resources are the required software and hardware that enables photo storage and sharing in the cloud or on-premise (traditional infrastructure).\n",
    "\n",
    "Looking at the graph, notice that traditional infrastructure doesn't scale when there are spikes in demand, and also leaves excess when preparing for future demand. This ability to easily meet unstable, fluctuating customer demand illustrates many of the benefits of cloud computing.\n",
    "\n",
    "##### Summary of Benefits of Risks Associated with Cloud Computing\n",
    "The capacity utilization graph above was initially used by cloud providers like Amazon to illustrate the benefits of cloud computing. Summarized below are the benefits of cloud computing that are often what drives businesses to include cloud services in their IT infrastructure. These same benefits are echoed in those provided by cloud providers Amazon (benefits), Google (benefits), and Microsoft (benefits).\n",
    "\n",
    "Benefits\n",
    "- Reduced Investments and Proportional Costs (providing cost reduction)\n",
    "- Increased Scalability (providing simplified capacity planning)\n",
    "- Increased Availability and Reliability (providing organizational agility)\n",
    "\n",
    "Below we have also summarized he risks associated with cloud computing. Cloud providers don't typically highlight the risks assumed when using their cloud services like they do with the benefits, but cloud providers like: Amazon (security), Google (security), and Microsoft (security) often provide details on security of their cloud services. It's up to the cloud user to understand the compliance and legal issues associated with housing data within a cloud provider's data center instead of on-premise. The service level agreements (SLA) provided for a cloud service often highlight security responsibilities of the cloud provider and those assumed by the cloud user.\n",
    "\n",
    "Risks\n",
    "- (Potential) Increase in Security Vulnerabilities\n",
    "- Reduced Operational Governance Control (over cloud resources)\n",
    "- Limited Portability Between Cloud Providers\n",
    "- Multi-regional Compliance and Legal Issues\n",
    "\n",
    "\n",
    "Though it is most common to use cloud computing resources for deployment, and local resources for many of the other steps of the machine learning process, it's possible to use cloud computing resources for any part of the machine learning process.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Building a Model using SageMaker\n",
    "\n",
    "\n",
    "### 3. Deploying and Using Model\n",
    "\n",
    "\n",
    "### 4. Hyperparameter Tuning\n",
    "\n",
    "\n",
    "### 5. Updating a Model\n",
    "\n",
    "\n",
    "### Project: Deploying a Sentiment Analysis Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d03c",
   "metadata": {},
   "source": [
    "## Projects\n",
    "\n",
    "- [Sentiment Analysis](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/Projects/00%20Sentiment%20Analysis)\n",
    "- [ImageClassifier](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/Projects/01%20Image%20Classifier)\n",
    "- [Predicting Bike-Sharing Patterns](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/Projects/02%20Bike-Sharing%20Patterns)\n",
    "- [Landmark Classification](https://github.com/stephengineer/Machine-Learning/tree/main/Deep%20Learning/Projects/03%20Landmark%20Classification%20and%20Tagging%20for%20Social%20Media)\n",
    "- [Dermatologist AI](https://github.com/udacity/dermatologist-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0eebeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
